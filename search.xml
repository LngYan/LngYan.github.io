<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello 博客</title>
    <url>/2023/01/10/Hello%20Blog/</url>
    <content><![CDATA[<p>我是初来的初学者，请多多指教！</p>
<p>这是我的第一篇博客。</p>
<p>记录下我的目标希望在回首看到这篇文章的时候我已经达成目标！</p>
<h3 id="高级数据分析师"><a class="markdownIt-Anchor" href="#高级数据分析师"></a> 高级数据分析师</h3>
<p>踏入数据分析的队伍是因为能对未来数据进行预测是一个很有意思的挑战。后来，我直接报了个班，花了半年时间系统学习了数据分析的基础理解了数据采集、数据清洗、数据分析、数据挖掘的方方面面，在对数据分析的深层次的了解后，认识到数据分析是展现价值，而数据的深层挖掘预测出的结果是通过相关的算法计算并且依靠机器学习不优化学习出来所以说算法及机器学习才是核心，数据是计算的基础。所以现阶段对于数据分析的目标是熟悉业务数据分析并且学习数据挖掘等各个计算基础等进一步提高自己的相关知识能力，在未来可以依靠自己的能力立足。</p>
<h3 id="做一个十万粉up主"><a class="markdownIt-Anchor" href="#做一个十万粉up主"></a> 做一个十万粉up主</h3>
<p>B站刚开始建站的时候我就注册了一个账号，一直用到现在，看着B站的起起落落，以前12dore团队不亦乐乎的MC整活在到纯黑、陆夫人、黑桐谷歌的各种游戏解说都是我小时候的乐趣，也是我想做UP主的一个起因，可我依旧没有规划好我该往那个方向（区）发展，感觉我什么都想做但是没都做不好，冷焱的出现就是最好的例子，所以我挺对不起冷焱的，想给他丰富的一生却法将精力全部投入，现在想做的事情和我的精力以及资金完全相反，我会一步一步的将这个目标敲定！</p>
<p>最后，就是对未来事业的遐想。对于最想进的大厂我还没有具体详细地去了解过，如果可以我希望自己可以成立一个小型企业，邀请一些志同道合地人来尽情地发挥自己的想象动用自己的能力。</p>
<p>最后最后，你要相信自己一定能行！</p>
]]></content>
      <categories>
        <category>blog</category>
        <category>target</category>
      </categories>
      <tags>
        <tag>初学者</tag>
        <tag>静态HTML</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 搭建入门</title>
    <url>/2023/01/31/Hexo%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%89%80%E8%B5%B0%E8%BF%87%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<h2 id="前沿"><a class="markdownIt-Anchor" href="#前沿"></a> 前沿</h2>
<h3 id="为何搭建博客"><a class="markdownIt-Anchor" href="#为何搭建博客"></a> 为何搭建博客</h3>
<p>建立博客的想法是在学习中一位老师推荐的，后来，随着个人知识面的拓展和深入，发现有太多的知识和经验值得沉淀和交流，在某年没有回家过春节的时候沉下心来搭建了一个博客，算是自己有一个回顾复盘的地方了吧。</p>
<p>另外，根据“学习金字塔”理论（Edgar Dale，1946），<strong>对知识进行演示、实践和传授能够显著提升学习保持率</strong>，而沉淀为博客或视频正是这样一种形式，我认为一方面可以让无形的知识转为有形的博客，便于之后复习和追溯，另一方面通过沉淀转化可以提高我对知识的记忆和掌握水平。</p>
<h3 id="建设原理"><a class="markdownIt-Anchor" href="#建设原理"></a> 建设原理</h3>
<p>博客自然是重内容，轻形式，主要重心点放在内容上，仅保留少量的交流功能即可。</p>
<p>这里我使用 <a href="https://hexo.io/">Hexo</a> 框架搭建博客，该框架基于 <a href="https://nodejs.org/zh-cn/">Node.js</a> ，使用者不需要掌握 node 原理，<a href="https://hexo.io/">Hexo</a> 会根据配置项中预设好的模板自动将内容转换为静态页面。</p>
<p>Hexo 提供了多种形式的主题：这里我选用的是 <a href="https://github.com/Fechin/hexo-theme-diaspora">Diaspora</a>。该主题较为贴近我对博客美观性的要求，且原生支持一系列配置项和插件，能满足我的需要。</p>
<p>虽然我有自己的NAS，但是为了提高其可用性，我还是白嫖了 GitHub Pages 服务，将 Hexo 生成的静态页面部署上去，从而可以让大家访问。</p>
<h2 id="基础环境安装"><a class="markdownIt-Anchor" href="#基础环境安装"></a> 基础环境安装</h2>
<h3 id="安装-nodejs"><a class="markdownIt-Anchor" href="#安装-nodejs"></a> 安装 Node.js</h3>
<p>直接到官网上下载安装即可https://nodejs.org/en/download/</p>
<ul>
<li>Node.js (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本)</li>
<li>Node自带npm</li>
</ul>
<p>安装完成后在终端运行如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ node -v</span><br><span class="line">$ npm  -v</span><br></pre></td></tr></table></figure>
<p>如果正常响应对应版本号，则说明安装及环境变量配置成功。</p>
<h3 id="安装git"><a class="markdownIt-Anchor" href="#安装git"></a> 安装Git</h3>
<p>安装Git需要用到安装器，我电脑上已安装HomeBrew所以可以直接安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew install git   // 安装Git</span><br></pre></td></tr></table></figure>
<p>也可以下载<a href="https://sourceforge.net/projects/git-osx-installer/">安装程序</a>来安装</p>
<h3 id="安装-hexo"><a class="markdownIt-Anchor" href="#安装-hexo"></a> 安装 Hexo</h3>
<p>使用 npm（node 的包管理器，可以类比 python 的 pip）可以直接安装 Hexo。在终端输入如下命令开始安装</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo npm install hexo-cli -g</span><br></pre></td></tr></table></figure>
<p>检查是否安装正常。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo -v</span><br></pre></td></tr></table></figure>
<p>安装后就可以开始配置了，选择一个本地文件夹作为博客的目录，终端切换到该目录下，初始化该目录并加载目录路径：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt;</span><br><span class="line">$ cd &lt;folder&gt;</span><br></pre></td></tr></table></figure>
<p>成功运行后，即可使用 hexo 的命令对该目录进行操作了</p>
<p><img src="/images/pasted-1.png" alt="upload successful" /><br />
运行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>即可从本地运行博客服务器。浏览器打开终端提示的页面(默认为 <a href="https://localhost:4000">https://localhost:4000</a>)，即可查看。</p>
<p>默认页面为内置的 <a href="http://hello-world.md">hello-world.md</a> 文档。至此我们已经完成了 <a href="https://hexo.io/zh-cn/">Hexo</a>的安装，如要了解更多配置项可参考<a href="https://hexo.io/zh-cn/api/">Configuration | Hexo</a>，要了解更多 Hexo 命令可参考 <a href="https://hexo.io/zh-cn/api/">Commands | Hexo</a>。</p>
<h3 id="主题安装"><a class="markdownIt-Anchor" href="#主题安装"></a> 主题安装</h3>
<p>为了使博客不太难看，我们需要安装一个主题就拿我目前用的<a href="https://github.com/Fechin/hexo-theme-diaspora">Diaspora</a>主题来举例，进入安装Hexo目录，安装主题：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cd &lt;folder&gt;</span><br><span class="line">$ git clone https://github.com/Fechin/hexo-theme-diaspora.git themes/diaspora</span><br></pre></td></tr></table></figure>
<p>安装完成后在theme文件夹内会多出一个名为Diaspora(刚刚安装的主题）文件夹就说明安装成功了<br />
接下来启动和设置主题，修改Hexo配置文件 <code>_config.yml</code> 主题项设置为diaspora</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">theme: diaspora</span><br></pre></td></tr></table></figure>
<p>如何设置主题可以在<a href="https://github.com/Fechin/hexo-theme-diaspora">Diaspora</a>里查询</p>
<p>注意：请在更时主题时备份 <code>_config.yml</code>配置文件</p>
<p>配置好主题后重新启动hexo你的博客就更新为你设置的主题啦</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo s</span><br></pre></td></tr></table></figure>
<p>当然Hexo还有非常多的主题供你选择。</p>
<h3 id="写文章"><a class="markdownIt-Anchor" href="#写文章"></a> 写文章</h3>
<p>所有基础框架都已经创建完成，接下来可以开始写你的第一篇博客了<br />
在<code>/source/_posts</code>下创建你的第一个博客吧，例如，创建一个名为<code>FirstNight.md</code>的文件，用Markdown大肆发挥吧，注意保存。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: First Night</span><br><span class="line">---</span><br><span class="line">&gt; 我有一头**小毛驴**，可是我从来都不骑。</span><br></pre></td></tr></table></figure>
<h2 id="发布"><a class="markdownIt-Anchor" href="#发布"></a> 发布</h2>
<p>测试没问题后，我们就生成静态网页文件发布至我们的Github pages 中或者自己的服务器上，国内的话可以发布到<a href="https://gitee.com/">gitee</a>上访问会快一些，毕竟git是国外网站，懂得都懂。<br />
<strong>在线发布</strong><br />
首先自己先注册个账号，注册过程可能需要验证你的邮箱，其他就不在赘述。</p>
<p>配置 Github 仓库：</p>
<p>需要创建一个仓库(repository) 来存储我们的网站，点击首页任意位置出现的 New repository按钮创建仓库, Respository name 中的<code>username.github.io</code>的username 一定与前面的Owner 一致，记住你的<code>username</code>下面会用到。</p>
<p><img src="/images/pasted-2.png" alt="upload successful" /></p>
<p>将 Hexo 文件夹中的文件 push 到储存库的默认分支，默认分支通常名为 main，旧一点的储存库可能名为 master。</p>
<ul>
<li>将 <strong>main</strong> 分支 push 到 GitHub：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push -u origin main</span><br></pre></td></tr></table></figure>
<ul>
<li>默认情况下 public/ 不会被上传(也不该被上传)，确保 .gitignore 文件中包含一行 public/。整体文件夹结构应该与 范例储存库 大致相似。</li>
</ul>
<p>使用 node --version 指令检查你电脑上的 Node.js 版本，并记下该版本 (例如：<code>v16.y.z</code>)<br />
在储存库中建立 <code>.github/workflows/pages.yml</code>（特别注意路径我就踩过坑把文件存在了根目录下），并填入以下内容 (将 16 替换为上个步骤中记下的版本)：</p>
<pre class="highlight"><code class="yml"><span class="hljs-string">.github/workflows/pages.yml</span>

<span class="hljs-attr">name:</span> <span class="hljs-string">Pages</span>
<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-string">main</span> <span class="hljs-comment"># default branch</span>
<span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">pages:</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Use</span> <span class="hljs-string">Node.js</span> <span class="hljs-number">16.</span><span class="hljs-string">x</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/setup-node@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">node-version:</span> <span class="hljs-string">"16"</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Cache</span> <span class="hljs-string">NPM</span> <span class="hljs-string">dependencies</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/cache@v2</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">node_modules</span>
          <span class="hljs-attr">key:</span> <span class="hljs-string">$&#123;&#123; runner.OS &#125;&#125;-npm-cache</span>
          <span class="hljs-attr">restore-keys:</span> <span class="hljs-string">|
            $&#123;&#123; runner.OS &#125;&#125;-npm-cache
</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">Dependencies</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">npm</span> <span class="hljs-string">install</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Build</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">npm</span> <span class="hljs-string">run</span> <span class="hljs-string">build</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Deploy</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">peaceiris/actions-gh-pages@v3</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">github_token:</span> <span class="hljs-string">$&#123;&#123; secrets.GITHUB_TOKEN &#125;&#125;</span>
          <span class="hljs-attr">publish_dir:</span> <span class="hljs-string">./public</span>
</code></pre>
<p>当部署作业完成后，产生的页面会放在储存库中的 gh-pages 分支。<br />
在储存库中前往 Settings &gt; Pages &gt; Source，并将 branch 改为 gh-pages。</p>
<p><img src="/images/pasted-3.png" alt="upload successful" /></p>
<p>过几分钟后，访问上方的网站，就可以看见你博客就搭建成功啦！</p>
<h2 id="更多"><a class="markdownIt-Anchor" href="#更多"></a> 更多</h2>
<p><a href="https://hexo.io/themes/">更多主题</a><br />
<a href="https://hexo.io/plugins/">更多插件</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
        <category>blog</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>个人网站</tag>
      </tags>
  </entry>
  <entry>
    <title>K邻近（KNN）算法</title>
    <url>/2023/03/10/K%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="knn分类模型"><a class="markdownIt-Anchor" href="#knn分类模型"></a> KNN分类模型</h1>
<p>k 近邻（k-Nearest Neighbor，kNN）是机器学习中相对简单且容易理解的算法，是分类数据最简单有效的算法，它基于实例学习，我们必须用最真实的样本数据对模型进行训练，以便预测出的结果更具参考意义。<br />
<img src="/images/K%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%951.PNG" alt="img" /><br />
简单地说，k 近邻算法采用测量不同特征值之间的距离方法进行分类。这句话简单也不简单。更通俗易懂的理解，所谓“物以类聚，人以群分”，kNN 认为彼此相隔最近的点为一类。周围的对象是什么类别，我就是什么类别。如果你的朋友都开法拉利、保时捷等跑车，那么在 kNN 眼你，你也是有钱人，也开豪车。</p>
<h2 id="工作原理"><a class="markdownIt-Anchor" href="#工作原理"></a> 工作原理</h2>
<p>存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据 与所属分类的对应关系。输人没有标签的新数据后，将新数据的每个特征与样本集中数据对应的 特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们 只选择样本数据集中前K个最相似的数据，这就是K-近邻算法中K的出处,通常K是不大于20的整数。 最后 ，选择K个最相似数据中出现次数最多的分类，作为新数据的分类。</p>
<blockquote>
<ul>
<li>对未知类型的数据集中的每个点依次执行如下操作：
<ul>
<li>计算已知类别数据集中的点与当前点之间的距离；</li>
<li>按距离递增次序排序；</li>
<li>选取与当前点距离最小的 k 个点；</li>
<li>确认前 k 个点出现频率最高的类别作为当前点的预测分类；</li>
</ul>
</li>
</ul>
</blockquote>
<p>接下来，举个栗子带大家更好的理解 kNN 算法。如图，有黄色的管理人才，深蓝色的架构师，以及未知的天蓝色。</p>
<p><img src="/images/K%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%952.PNG" alt="img" /></p>
<p>当我们需要对员工进行技术和管理两个培养方向分类时，可能会考虑到他们的情商、沟通、技能、协作、远见、解决问题等能力。由于人类大脑的限制，我们只能处理三维以下的事务，这里只列举两个特征“情商”和“技能”评分，假设情商分高可作为管理人才来培养，技能分高作为架构师人才来培养。</p>
<p>我们拿到了七个样本数据，每一个样本带有情商和技能特征，以及对应的标签，他们分布在图中的数据如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">技能</th>
<th style="text-align:center">情商</th>
<th style="text-align:center">培养方向</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">43</td>
<td style="text-align:center">92</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">60</td>
<td style="text-align:center">88</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">69</td>
<td style="text-align:center">80</td>
<td style="text-align:center">架构师</td>
</tr>
<tr>
<td style="text-align:center">66</td>
<td style="text-align:center">78</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">88</td>
<td style="text-align:center">50</td>
<td style="text-align:center">架构师</td>
</tr>
<tr>
<td style="text-align:center">80</td>
<td style="text-align:center">91</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">96</td>
<td style="text-align:center">35</td>
<td style="text-align:center">架构师</td>
</tr>
<tr>
<td style="text-align:center">68</td>
<td style="text-align:center">80</td>
<td style="text-align:center">？</td>
</tr>
</tbody>
</table>
<p>现在的需求是通过天蓝色的特征值预测他更适合的培养方向，按照 kNN 算法的实现步骤，计算出天蓝色特征与样本集的距离。计算二维平面上两点 a(x1,y1) 与 b(x2,y2) 间的欧氏距离</p>
<p><img src="/images/K%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%953.PNG" alt="img" /><br />
按距离升序排序之后结果如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">技能</th>
<th style="text-align:center">情商</th>
<th style="text-align:center">欧式距离↑</th>
<th style="text-align:center">培养方向</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">69</td>
<td style="text-align:center">80</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">架构师</td>
</tr>
<tr>
<td style="text-align:center">66</td>
<td style="text-align:center">78</td>
<td style="text-align:center">2.83</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">60</td>
<td style="text-align:center">88</td>
<td style="text-align:center">11.31</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">80</td>
<td style="text-align:center">91</td>
<td style="text-align:center">16.27</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">43</td>
<td style="text-align:center">92</td>
<td style="text-align:center">27.73</td>
<td style="text-align:center">管理</td>
</tr>
<tr>
<td style="text-align:center">88</td>
<td style="text-align:center">50</td>
<td style="text-align:center">36.06</td>
<td style="text-align:center">架构师</td>
</tr>
<tr>
<td style="text-align:center">96</td>
<td style="text-align:center">35</td>
<td style="text-align:center">53.0</td>
<td style="text-align:center">架构师</td>
</tr>
</tbody>
</table>
<p>我们得到了预测数据与各个样本间的距离，并按递增排好序。现假设 k = 3，对应的类别分别是“架构师”、“管理”、“管理”，按照算法实现的第 4 步，前 3 个点出现频率最高的类别是“管理”。那么，天蓝色的培养方向应该偏管理。</p>
<p>实例中 k 值取多少没有定论，如果实例中 k = 1 或 k = 2，那么结果未必是最准确的，这是一个经验值。通常 k 是 3 ≤ k &lt; 20 的整数，并且是奇数，这样避免出现相同票数。</p>
<p>回过头来看数据分布图，以你为中心画圆，其中圆 c1 也就是 k = 1，距你最近的是“架构师”，圆 c2 则是当 k = 3 的时候，其中三个“管理”，少数服从多数，统计最多的类别依然是“管理”。</p>
<h2 id="knn优缺点"><a class="markdownIt-Anchor" href="#knn优缺点"></a> KNN优缺点</h2>
<p><strong>优点：精度高、对异常值不敏感、无数据输入假定。</strong><br />
缺点：kNN 必须保存全部数据集，会占用大量的存储空间，所以空间负责度高；<strong>kNN 必须对数据集中的每个数据计算距离值，实际使用时非常耗时，所以计算复杂度高。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在scikit-learn库中使用k-近邻算法</span></span><br><span class="line">   <span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br></pre></td></tr></table></figure>
<h1 id="交叉验证"><a class="markdownIt-Anchor" href="#交叉验证"></a> 交叉验证</h1>
<ul>
<li>学习曲线&amp;交叉验证选取K值
<ul>
<li>K值较小，则模型复杂度较高，容易发生过拟合，学习的估计误差会增大，预测结果对近邻的实例点非常敏感。</li>
<li>K值较大可以减少学习的估计误差，但是学习的近似误差会增大，与输入实例较远的训练实例也会对预测起作用，使预测发生错误，k值增大模型的复杂度会下降。</li>
<li>在应用中，k值一般取一个比较小的值，通常采用交叉验证法来来选取最优的K值。</li>
</ul>
</li>
<li>适用场景
<ul>
<li>小数据场景，样本为几千，几万的</li>
</ul>
</li>
<li>目的：
<ul>
<li>选出最为适合的模型超参数的取值，然后将超参数的值作用到模型的创建中。</li>
</ul>
</li>
<li>思想：
<ul>
<li>将样本的训练数据交叉的拆分出不同的训练集和验证集，使用交叉拆分出不同的训练集和验证集测分别试模型的精准度，然就求出的精准度的均值就是此次交叉验证的结果。将交叉验证作用到不同的超参数中，选取出精准度最高的超参数作为模型创建的超参数即可！</li>
</ul>
</li>
<li>实现思路：
<ul>
<li>将训练数据平均分割成K个等份</li>
<li>使用1份数据作为验证数据，其余作为训练数据</li>
<li>计算验证准确率</li>
<li>使用不同的测试集，重复2、3步骤</li>
<li>对准确率做平均，作为对未知数据预测准确率的估计</li>
</ul>
</li>
</ul>
<p><img src="/images/K%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%954.PNG" alt="img" /></p>
<blockquote>
<ul>
<li>API
<ul>
<li>from sklearn.model_selection import cross_val_score</li>
<li>cross_val_score(estimator,X,y,cv):
<ul>
<li>estimator:模型对象</li>
<li>X,y:训练集数据</li>
<li>cv：折数</li>
</ul>
</li>
</ul>
</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>数据挖掘</category>
        <category>机器学习</category>
        <category>KNN</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据挖掘</tag>
        <tag>机器学习</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title>了解chatGPT</title>
    <url>/2023/02/12/%E4%BA%86%E8%A7%A3%E7%88%86%E7%81%AB%E7%9A%84chatGPT/</url>
    <content><![CDATA[<p>近期chatGPT刷爆的所有人的首页、社交平台、朋友圈，如此火爆的背后是人工智能发展出萌芽的第一步，所以我也去网上收集了许多资料，会在这里分享给大家。</p>
<p><img src="/images/chatGPT7.jpg" alt="img" /></p>
<h2 id="chatgpt是什么"><a class="markdownIt-Anchor" href="#chatgpt是什么"></a> 【ChatGPT是什么？】</h2>
<p>首先，ChatGPT 是由OpenAI公司在2022年11月30日发布的一种聊天机器人模型，是由人工智能技术驱动的自然语言处理工具，其中 Chat 就是聊天。当然不是像小爱同学、siri等智能家居系统这样简单的回答以及完型填空，chatGPT更像是回答你出的命题作文一样进行写作。</p>
<p>然后重点是后面的 <strong>【GPT】</strong> ，即 <strong>Generative Pre-trained Transformer</strong> ，中文叫 <strong>“生成型预训练变换模型”</strong>。<br />
简单来说，ChatGPT就像我们手机上的一款APP，不同之处在于，他能学习和理解人类的语言，还能根据上下文的语境进行反应，真正像人一样和你进行交流，同时他一门应用不是技术，并且chatGPT最重要的不是技术而是工程。</p>
<p>不仅仅只是聊天的chatGPT还能：</p>
<ul>
<li>[✓] 写代码、修bug</li>
<li>[✓] 设计装修图</li>
<li>[✓] 写剧本，写论文</li>
<li>[✓] 解析物理题<br />
如果你的提问当中出现明显的错误，他还会纠正你。</li>
</ul>
<h2 id="chatgpt的由来"><a class="markdownIt-Anchor" href="#chatgpt的由来"></a> 【chatGPT的由来】</h2>
<p>2017年，谷歌大脑团队（Google Brain）在神经信息处理系统大会（NeurIPS，该会议为机器学习与人工智能领域的顶级学术会议）发表了一篇名为“Attention is all you need”（自我注意力是你所需要的全部）的论文。作者在文中首次提出了基于自我注意力机制（self-attention）的变换器（transformer）模型，并首次将其用于理解人类的语言，即自然语言处理。</p>
<p>在这篇文章面世之前，自然语言处理领域的主流模型是循环神经网络（RNN，recurrent neural network）。循环神经网络模型的优点是，能更好地处理有先后顺序的数据，比如语言，但也因为如此，这种模型在处理较长序列，例如长文章、书籍时，存在模型不稳定或者模型过早停止有效训练的问题（这是由于模型训练时的梯度消失或梯度爆炸现象而导致，在此不具体展开），以及训练模型时间过长（因必须顺序处理数据，无法同时并行训练）的问题。</p>
<p>2015年12月，OpenAI公司美国旧金山成立。特斯拉的创始人马斯克也是该公司创始人之一，为公司早期提供了资金支持（后来他从该公司退出，但保留了金主身份，并未撤资）。成立早期，OpenAI是一家非营利组织，以研发对人类社会有益、友好的人工智能技术为使命。2019年，OpenAI改变了其性质，宣布成为营利机构，这个改变与Transformer模型不无相关。</p>
<p>2018年，在Transformer模型诞生还不到一年的时候，OpenAI公司发表了论文“Improving Language Understanding by Generative Pre-training”（用创造型预训练提高模型的语言理解力）（Generative一般译为“生成型”，但我认为译为“创造型”更合适），推出了具有1.17亿个参数的GPT-1（Generative Pre-training Transformers, 创造型预训练变换器）模型。这是一个用大量数据训练好的基于Transformer结构的模型。他们使用了经典的大型书籍文本数据集（BookCorpus）进行模型预训练。该数据集包含超过7000本从未出版的书，类型涵盖了冒险、奇幻、言情等类别。在预训练之后，作者针对四种不同的语言场景、使用不同的特定数据集对模型进行进一步的训练（又称为微调，fine-tuning）。最终训练所得的模型在问答、文本相似性评估、语义蕴含判定、以及文本分类这四种语言场景，都取得了比基础Transformer模型更优的结果，成为了新的业内第一。<br />
<img src="/images/chatGPT1.jpg" alt="img" /></p>
<p>2019年，该公司公布了一个具有15亿个参数的模型：GPT-2。该模型架构与GPT-1原理相同，主要区别是GPT-2的规模更大（10倍）。同时，他们发表了介绍这个模型的论文“Language Models are Unsupervised Multitask Learners” （语言模型是无监督的多任务学习者）。在这项工作中，他们使用了自己收集的以网页文字信息为主的新的数据集。不出意料，GPT-2模型刷新了大型语言模型在多项语言场景的评分记录。在文中，他们提供了GPT-2模型回答新问题（模型训练数据中未出现过的问题及其答案）的结果。</p>
<p>2020年，这个创业团队再次战胜自己，发表论文“Language Models are Few-ShotLearner”（语言模型是小样本学习者），并推出了最新的GPT-3模型——它有1750亿个参数。GPT-3模型架构与GPT-2没有本质区别，除了规模大了整整两个数量级以外。GPT-3的训练集也比前两款GPT模型要大得多：经过基础过滤的全网页爬虫数据集（4290亿个词符）、维基百科文章（30亿词符）、两个不同的书籍数据集（一共670亿词符）。</p>
<p>由于巨大的参数数目以及训练所需数据集规模，训练一个GPT-3模型保守估计需要五百万美元至两千万美元不等——如果用于训练的GPU越多，成本越高，时间越短；反之亦然。可以说，这个数量级的大型语言模型已经不是普通学者、一般个人能负担得起研究项目了。面对如此庞大的GPT-3模型，用户可以仅提供小样本的提示语、或者完全不提供提示而直接询问，就能获得符合要求的高质量答案。小样本提示是指用户在提问时先给模型提供几个例子，然后再提出自己的语言任务（翻译、创作文本、回答问题等）。</p>
<p><img src="/images/chatGPT2.jpg" alt="img" /></p>
<p>GPT-3模型面世时，未提供广泛的用户交互界面，并且要求用户提交申请、申请批准后才能注册，所以直接体验过GPT-3模型的人数并不多。根据体验过的人们在网上分享的体验，我们可以知道GPT-3可以根据简单的提示自动生成完整的、文从字顺的长文章，让人几乎不能相信这是机器的作品。GPT-3还会写程序代码、创作菜谱等几乎所有的文本创作类的任务。早期测试结束后，OpenAI公司对GPT-3模型进行了商业化：付费用户可以通过应用程序接口（API）连上GPT-3，使用该模型完成所需语言任务。2020年9月，微软公司获得了GPT-3模型的独占许可，意味着微软公司可以独家接触到GPT-3的源代码。该独占许可不影响付费用户通过API继续使用GPT-3模型。</p>
<p>2022年3月，OpenAI再次发表论文“Training language models to follow instructions with human feedback”（结合人类反馈信息来训练语言模型使其能理解指令），并推出了他们基于GPT-3模型并进行了进一步的微调的InstructGPT模型。InstructGPT的模型训练中加入了人类的评价和反馈数据，而不仅仅是事先准备好的数据集。</p>
<p>GPT-3公测期间用户提供了大量的对话和提示语数据，而OpenAI公司内部的数据标记团队也生成了不少人工标记数据集。这些标注过的数据（labelled data），可以帮助模型在直接学习数据的同时学习人类对这些数据的标记（例如某些句子、词组是不好的，应尽量少使用）。</p>
<p>OpenAI公司第一步先用这些数据对GPT-3用监督式训练（supervised learning）进行了微调。</p>
<p>第二步，他们收集了微调过的模型生成的答案样本。一般来说，对于每一条提示语，模型可以给出无数个答案，而用户一般只想看到一个答案（这也是符合人类交流的习惯），模型需要对这些答案排序，选出最优。所以，数据标记团队在这一步对所有可能的答案进行人工打分排序，选出最符合人类思考交流习惯的答案。这些人工打分的结果可以进一步建立奖励模型——奖励模型可以自动给语言模型奖励反馈，达到鼓励语言模型给出好的答案、抑制不好的答案的目的，帮助模型自动寻出最优答案。</p>
<p>第三步，该团队使用奖励模型和更多的标注过的数据继续优化微调过的语言模型，并且进行迭代。最终得到的模型被称为InstructGPT。</p>
<p><img src="/images/chatGPT4.jpg" alt="img" /><br />
<img src="/images/chatGPT3.jpg" alt="img" /></p>
<h2 id="chatgpt的商业价值"><a class="markdownIt-Anchor" href="#chatgpt的商业价值"></a> 【ChatGPT的商业价值】</h2>
<p>在Open AI公司推出chatGPT程序之后，国内外的科技巨头都坐不住了：</p>
<ul>
<li>谷歌：自己也将推出同类型的AI聊天机器人；</li>
<li>苹果：马上通过彭博社宣布，将在近期举行AI峰会；</li>
<li>百度：正式官宣了公司类chatGPT项目“文心一言”，可以说是国内在人工智能领域技术积淀最深的企业；</li>
<li>阿里巴巴：阿里达摩院研发的类似对话机器人，已经在开放给内部员工进行内测的阶段；<br />
…</li>
</ul>
<p>说到底，ChatGPT本身是一个现象，现象背后的人工智能趋势，才是真正最重要的。</p>
<p>科技巨头们急了，是因为他们深怕错过了这一波人工智能浪潮的红利。</p>
<p>OpenAI推出ChatGPT付费订阅版ChatGPTPlus，每月收费20美元，开启产品走向商业化变现道路。</p>
<p>随着智能客服、教育、医疗、搜索引擎等应用领域不断落地，ChatGPT将与各行业应用结合后，更多付费商业模式即将落地AI人工智能的赛道将变的越来越壮大。</p>
<p>这里不得不提到AIGC，即人工智能生成内容，ChatGPT、AI 绘画、AI数字虚拟人、AI智能客服等都是AIGC的范畴，将是未来人工智能的重要发展方向。<br />
<img src="/images/chatGPT8.jpg" alt="img" /></p>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>ChatGPT能够促使AIGC快速商业化发展，预示着自然语言处理（NLP）技术有望迅速进入平民化应用时代，应用场景不断拓宽…</p>
<p>所以ChatGPT的一小步，是AI的一大步!</p>
<p>ChatGPT也引发了很多人的焦虑和失业危机：</p>
<blockquote>
<p>① 简单的数据处理和录入工作者；<br />
② 简单的客服工作者；<br />
③ 简单的金融分析和记账工作者；<br />
④ 重复性高的生产线工作；</p>
</blockquote>
<p>当然，有些岗位可能会因此得到提升，例如:</p>
<blockquote>
<p>① 人工智能开发人员;<br />
② 数据分析师;<br />
③ 自动化工程师;<br />
④ 用户体验设计师等。</p>
</blockquote>
<p>以及在2023年2月出台的规范使用，媒体报道称，欧盟负责内部市场的委员蒂埃里·布雷东日前就“聊天生成预训练转换器”发表评论说，这类人工智能技术可能为商业和民生带来巨大的机遇，但同时也伴随着风险，因此欧盟正在考虑设立规章制度，以规范其使用，确保向用户提供高质量、有价值的信息和数据。</p>
<p>随着法律规范，商业应用的陆续提上日程等一系列信息，都表明AI的发展将进入到加速阶段，那么AI究竟是何发展？让我们拭目以待吧！</p>
<p><strong>最后呢还是要强调下，ChatGPT本身是一个现象，现象背后的人工智能趋势，才是真正最重要的。</strong></p>
<h2 id="更多"><a class="markdownIt-Anchor" href="#更多"></a> 更多</h2>
<blockquote>
<p>人工智能的两条基础理念：<br />
1、有内在规律，有概率的、不能百分百保证预测正确。<br />
2、可采集数据，可以做随机数据获取大量机器学习的基础。</p>
</blockquote>
<blockquote>
<p>人工智能的两个过程：<br />
1、训练：从老数据中挖掘数据的规律。<br />
2、预测：讲挖掘到的规律运用到新数据中。</p>
</blockquote>
<blockquote>
<p>人工智能优势与劣势：<br />
1、强项：规则比较模糊的场景<br />
2、弱项：规则比较清晰的场景</p>
</blockquote>
<blockquote>
<p>初入人工智能项目流程：<br />
1、确认好输入输出<br />
2、根据输入输出，采集数据<br />
3、遴选算法，完成训练<br />
4、模型部署，上线推理</p>
</blockquote>
<p>加减乘除不适合人工智能来做，计算器来算才是百分百正确。</p>
<p><strong>人工智能就是拥有“人”的特点。</strong></p>
<p><img src="/images/chatGPT5.jpg" alt="img" /><br />
<img src="/images/chatGPT6.jpg" alt="img" /></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>chatGPT</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>chatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>如何注册chatGPT</title>
    <url>/2023/02/13/%E5%A6%82%E4%BD%95%E6%B3%A8%E5%86%8CchatGPT/</url>
    <content><![CDATA[<h2 id="注册"><a class="markdownIt-Anchor" href="#注册"></a> 注册</h2>
<p>1、魔法上网，可以使用国外的节点，<strong>但不能是香港、澳门、台湾等的节点</strong>。日本、美国、新加坡等区域亲测可以（看网络上不少网友反馈港澳台和部分地区会被openAI封锁，注册前请确认自己的环境）。<br />
2、国外的手机号码，同样地区也是如上述网络环境之外的地区手机号（注意部分虚拟手机号的厂家无法接受验证码，请注意甄别）。</p>
<h2 id="流程"><a class="markdownIt-Anchor" href="#流程"></a> 流程</h2>
<p>1.打开ChatGPT的官方网站，链接为https://chat.openai.com，然后点击【Sign Up】进入下一步。<br />
如果页面无法正常显示，需要更换其他的网络节点，或者清理下浏览器cookie再次尝试。<br />
<img src="/images/%E6%B3%A8%E5%86%8CchatGPT1.png" alt="img" /><br />
2.注册方式为邮箱注册，可以用微软帐号或者谷歌帐号登录，微软账号或者谷歌账号好像不需要验证，国内或者其他邮箱地址都需要进行验证（比如QQ邮箱是需要验证的）。<br />
<img src="/images/%E6%B3%A8%E5%86%8CchatGPT2.png" alt="img" /><br />
3.邮箱通过后，会提示你输入姓名，按照要求进行输入即可。如果显示该IP地址注册数量过多，则需要更换节点（更换节点时，无需对浏览器进行重启，刷新页面即可）。<br />
<img src="/images/%E6%B3%A8%E5%86%8CchatGPT3.png" alt="img" />）<br />
3.随后将会进入手机验证的环节，记住：这里不能选择国内的手机号，国内的手机号无法进行注册。<br />
<img src="/images/%E6%B3%A8%E5%86%8CchatGPT4.png" alt="img" /><br />
这里比较推荐俄罗斯的<a href="https://sms-activate.org">sms-activate</a>虚拟服务平台实时帮你接收验证码，非常快速而且里面不仅能接收open AI，其他平台也都能注册，强推！！！<br />
<img src="/images/%E6%B3%A8%E5%86%8CchatGPT6.png" alt="img" /><br />
注意，在接收验证码时，建议使用低价的印尼等手机号，印度经常挤爆收不到；如果长时间没有收到验证码，可更换成其他区域尝试。<br />
重点是：在手机验证页面，一定要注意选择正确的国家（注意国旗）！一定要注意选择正确的国家！一定要注意选择正确的国家！<br />
4.注册完毕后，就可以自由自在的使用ChatGPT了。<br />
<img src="/images/%E6%B3%A8%E5%86%8CchatGPT5.png" alt="img" /></p>
]]></content>
      <categories>
        <category>人工智能</category>
        <category>chatGPT</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>chatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>描述统计及推断统计</title>
    <url>/2023/02/07/%E6%8F%8F%E8%BF%B0%E7%BB%9F%E8%AE%A1%E5%8F%8A%E6%8E%A8%E6%96%AD%E7%BB%9F%E8%AE%A1/</url>
    <content><![CDATA[<h1 id="什么是统计学"><a class="markdownIt-Anchor" href="#什么是统计学"></a> 什么是统计学</h1>
<p>首先统计学是数据分析师必走的一条道路，它可以从科学的角度上说明数的收集、处理、分析和解释，并从数据中得出结论。当然最先遇到的难题就是映入眼前的一大堆让人懵逼的复杂数学公式，直接让人还没开始就直接开摆，但实际学习下来过后发现统计学就在我们身边也没有那么的难，也不一定要会很多公式的推导，我们首先是先理解，从应用层面上，然后再慢慢深入研究，知道他并且会用是我们的第一步，我们先将个最通俗易懂的例子：</p>
<blockquote>
<p>案例1：假设一家公司的老板说自己公司平均工资在20000左右，你是不是很心动？但实际上可能老板 60000 张三 6000 李四<br />
8000 王五 5000 所以你去过可能也就是5000～7000的水平可见这个坑有多深。<br />
案例二：2019年国家统计局发布的工资数据中提到：信息传输、软件和信息技术服务业平均工资得以快速增⻓，2019年平均工资为122478元，比上年增⻓9.3%</p>
</blockquote>
<p><img src="/images/%E6%8F%8F%E8%BF%B0%E7%BB%9F%E8%AE%A1%E5%8F%8A%E6%8E%A8%E6%96%AD%E7%BB%9F%E8%AE%A11.jpg" alt="img" /></p>
<p>这么看是不是发现统计学他时时刻刻都在我们身边？将他啃下，我们也能当一名会做生意的老板。<br />
大概讲明白了统计学的的例子，我们继续把统计学中两个重要的分支：描述统计和推断统计，在拿出来说说。</p>
<h2 id="描述统计"><a class="markdownIt-Anchor" href="#描述统计"></a> 描述统计</h2>
<ul>
<li>将一系列复杂数据，减少为几个能起到描述作用的数字（均值，中位数等），用这些有代表性的数字来代表整个数字集。然后还可以将代表性数据图形化/可视化，可以更直观的了解数据，从而用数据解释问题。</li>
<li>例如：说一下班级这次考试的情况如何</li>
</ul>
<p>那描述统计中又有：</p>
<ul>
<li>集中描述统计</li>
<li>离散趋势统计</li>
</ul>
<p>下面先讲解下集中描述统计和离散趋势统计</p>
<h3 id="集中描述统计"><a class="markdownIt-Anchor" href="#集中描述统计"></a> 集中描述统计</h3>
<p><strong>反映数据向其中心值靠拢或聚集的程度</strong></p>
<h4 id="均数"><a class="markdownIt-Anchor" href="#均数"></a> 均数</h4>
<p>描述一组数据在数量上的平均水平，主要用于数值型数据</p>
<ul>
<li>均数的优点：
<ul>
<li>高度浓缩了数据的精华，使大量的观测数据转变成一个代表性的数值；比较敏感，数据任何一个值发生变化，均数都会随之改变。</li>
<li>大家熟知、都比较喜欢用、便于比较和传播。</li>
</ul>
</li>
<li>均数的缺点：
<ul>
<li>大锅饭，把各个观测数据之间的差异性掩盖了</li>
<li>均数受极值的影响很大。</li>
<li>上文提到的平均工资的事情，就是均值的缺点造成的</li>
</ul>
</li>
<li>均数的使用范围
<ul>
<li>对称分布，特别是正态分布的数据比较适用，但是对于极端性数据均数绝对不适用</li>
</ul>
</li>
<li>pandas计算均值：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.mean(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="中位数"><a class="markdownIt-Anchor" href="#中位数"></a> 中位数</h4>
<p>主要用于顺序型数据。将全体数据从小到大排列，在整个数列中处于中间位置的那个值就是中位数。个数为奇数的中位数，根据位置取中间的那个数即可。个数为偶数的中位数，就是中间两个数值的均值。</p>
<ul>
<li>中位数的优点
<ul>
<li>不受极端值的影响，在具有个别极大或极小值的分布数列中，中位数比均数更具有代表性，如上面例子，用中位数则是4500，至少代表了前三个人的工资水平</li>
</ul>
</li>
<li>中位数的缺点
<ul>
<li>损失信息，只考虑居中位置，其他变量值比中位数大多少或小多少，它无法反映出来，所以我们也是只能看到部分信息。</li>
</ul>
</li>
<li>中位数的应用场景：
<ul>
<li>对于对称性的数据，优先均数，仅仅对于均数不能使用的情况才使用中位数加以描述。</li>
<li>在描述数据的时候，我们通常会通过平均数与中位数来认识数据。但有些时候，哪怕我们准确无误的计算出了平均数，也无法改变中位数在对真相的描述中更加准备这个事实，为什么这么说呢？
<ul>
<li>首先，我们关心的多大数现象都可以用多种方式进行描述。但同样的，当我们在对同一事物进行描述的时候，我们说的话（选用的数据）便会影响别人对此事的印象。</li>
<li>比如3、4、5、6、102这5个数字，它们的平均数是24，而中位数是5。很明显，24和5之间存在明显的差距。有些时候，你想让整体的数字看起来更大，就用平均数。如果你想让整体数字看起来更小，那就用中位数。这时，就很容易误导看数据的人。因为一看到你的平均数这么大，很自然的就会认为你的东⻄不错。其实不是的。因为这组数据的平均数之所以这么大，完全是因为102这个极值拉大了整体的均值，所以平均数看起来还错。只是这样的话，便会对数据产生一定的误解。因此，从这个⻆度出发，当我们要准确的认识数据的时候，要尽可能避免使用平均数来作为判断对象好坏的唯一标准，我们得加入中位数数据，使得结论更加准确。</li>
</ul>
</li>
</ul>
</li>
<li>pandas计算中位数/四分位数：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = DataFrame(data=np.random.randint(<span class="number">0</span>,<span class="number">100</span>,size=(<span class="number">3</span>,<span class="number">5</span>)))</span><br><span class="line">df.median(axis=<span class="number">1</span>) <span class="comment">#计算df中每一行的中位数，如果行数据不是有序的，</span></span><br><span class="line">则median会将其变为有序在计算中位数</span><br><span class="line">df.describe() <span class="comment">#可以将df的每一列进行统计描述</span></span><br></pre></td></tr></table></figure>
<h4 id="众数"><a class="markdownIt-Anchor" href="#众数"></a> 众数</h4>
<ul>
<li>众数是指总体中出现次数最多的标志值，简单来说就是一组数据当中，出现次数最多的那个数。在实际工作中，众数有相当广泛的应用。
<ul>
<li>例如，市场上某种商品一天的价格可能有多次变化，可不必全面登记该商品的全部价格来计算其算术平均数，而只需用该商品成交量最多的那个价格即众数作为代表值，就可以反映出该商品价格的一般水平。</li>
<li>又如，在大批量生产的男式皮鞋中有多种尺码，其中40码的销售量最多，这说明40码就是众数，可代表男式皮鞋尺码的一般水平，宜大量生产，而其余尺码的生产量就要相应少一些，这样才能满足市场上大部分消费者的需要。</li>
</ul>
</li>
<li>pandas计算众数：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">s.mode() <span class="comment">#众数</span></span><br></pre></td></tr></table></figure>
<h3 id="离散趋势描述"><a class="markdownIt-Anchor" href="#离散趋势描述"></a> 离散趋势描述</h3>
<h4 id="方差"><a class="markdownIt-Anchor" href="#方差"></a> 方差</h4>
<ul>
<li>案例分析：
<ul>
<li>要从甲乙两名跳远运动员中选出一名去参加运动会，为此专⻔为两人进行了10次跳远比赛，产出了两个人各自10组跳远的成绩记录。选拔标准是，先看他们的平时成绩，如果平时成绩相差无几的话，在看稳定程度。那么，就可以使用方差来衡量其稳定程度。下面来看下，如何一步一步的推导出方差。</li>
</ul>
</li>
<li>提问：如果用一组数据的平均数来代表样本平均水平的话，对个体(每个数据)而言，什么指标可以代表个体(每个数据)的离散程度大小？（一次跳远成绩距离均值的距离）
<ul>
<li>可以使用离均差：x-μ（个体偏离均值的程度）</li>
</ul>
</li>
<li>提问：可否用离均差的总和来表示整个样本的离散程度？不可以，离均差有正负之分，加和会抵消为0。那怎么办，怎么解决正负号的问题？可以<strong>使用离均差的平方和：∑(x-μ)²</strong><br />
但是：<br />
如果比较两个样本的离均差，一个样本量是10个，一个是1000个，实际上二者的离散程度是一样的，但是因为数量不同，造成平方和相加和数值差异很大，这该怎么办？</li>
<li>显然，我们发现离均差平方和的大小跟样本量有关</li>
<li>如果我们能够把离均差平方和/样本量，是不是就解决了这个问题，其实这个就是方差的概念<br />
方差：反映各数据远离其中心值的趋势<br />
pandas计算方差：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">s.var()</span><br></pre></td></tr></table></figure>
<p>标准差<br />
方差开根号就是标准差</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">s.std()</span><br></pre></td></tr></table></figure>
<p>思考：<br />
一组数据携带的信息量的大小可以使用方差来衡量方差越大表示这组数据携带的信息量越大，反之越小。</p>
<ul>
<li>第一组数据：9, 9.1, 9.01, 8.99， 9.1 方差几乎为0</li>
<li>第二组数据：9 102 22 1 0.8 方差较大</li>
</ul>
<h2 id="推断统计"><a class="markdownIt-Anchor" href="#推断统计"></a> 推断统计</h2>
<p>是研究如何利用样本数据来推断总体特征的统计方法。推断统计其实是建立在描述统计的基础之上，在对总体数据有了大致的了解之后，运用一些分析方法，对数据进行预测，并达到统计决策的目的，其实不管是在统计学上，还是在实际的业务分析中，我们做分析的终极目的就是用来得出我们结论，应用于决策。</p>
<ul>
<li>例如：房价预测，通过预测数据来进行销售，用户看到房价走势，如果一路走高，是不是要提早下手。</li>
</ul>
<p><img src="/images/%E6%8F%8F%E8%BF%B0%E7%BB%9F%E8%AE%A1%E5%8F%8A%E6%8E%A8%E6%96%AD%E7%BB%9F%E8%AE%A12.jpg" alt="img" /></p>
<hr />
<p>看完这个篇文章的内容后是否开始一点点的了解？如果还是没有了解也没关系，我们更多的是要了解如何应用进pandas当中，学习他们在分析中的作用，来帮助我们更好的完成所需要的工作。</p>
<p>之后我会找时间讲述下统计学中的几个概念，二项分布和正太分布，拜~</p>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
        <tag>基础</tag>
        <tag>描述统计</tag>
        <tag>推断统计</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析之Excel</title>
    <url>/2023/02/05/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel/</url>
    <content><![CDATA[<p>在一个数据驱动运营、数据决定对策、数据改变未来的时代。无论是海量数据库，还是一张简单的表格，都能进一步挖掘数据价值、活用数据。在众多数据分析工具中，Excel 是最常用，也是最容易上手的分析工具。<br />
<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel1.jpg" alt="img" /><br />
Excel 数据分析功能十分强大，不仅提供简单的数据处理功能，还有专业的数据分析工具库，包括相关系数分析、描述统计分析等。</p>
<h2 id="为什么选择用excel"><a class="markdownIt-Anchor" href="#为什么选择用excel"></a> 为什么选择用Excel？</h2>
<p>1.Excel是非常常见普遍存在的工具不仅能完成日常常见的大部分内容，同时也是便捷的数据分析工具。功能非常强大有数据统计、函数计算、数据透视表、图表等功能。可以满足一般的数据分析需求。<br />
2.Excel简单易学。对于初学者来说，Excel从入门到精通，只要你每天花点时间学习及练习，一个月就可以掌握。<br />
3.做为office三剑客之一，Excel、Word、PPT的江湖地位是无可撼动的。 当然还有更多更专业的数据分析软件如Spss、Python等，也可以实现数据分析。 另外越来越多的公司建立了BI报表体系，用来做数据分析和展示，当然这个是需要一定的资金支持。</p>
<p>是的，就是你看不起的Excel，它能完成的大部分的事情包含但不限于：</p>
<blockquote>
<ul>
<li>数据清洗（完整、合法、唯一）</li>
<li>描述性统计分析（数据表现一秒生成）</li>
<li>变化和趋势分析（多维度分析、交叉分析）</li>
<li>回归和预测（数据之间内在关系有多可信）</li>
</ul>
</blockquote>
<h2 id="excel做数据分析方法"><a class="markdownIt-Anchor" href="#excel做数据分析方法"></a> Excel做数据分析方法</h2>
<p><img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel3.jpg" alt="img" /></p>
<h3 id="准备数据并生成透视表"><a class="markdownIt-Anchor" href="#准备数据并生成透视表"></a> 准备数据并生成透视表</h3>
<p>在excel数据中，将清洗完成的数据选中→插入→数据透视表，把地区、省份（需要汇总的维度）放到行，利润、销售额（需要汇总的数值）放到值。</p>
<h3 id="设计格式"><a class="markdownIt-Anchor" href="#设计格式"></a> 设计格式</h3>
<p>选中excel顶部菜单的设计→报表布局→已表格形式显示→分类汇总→不显示分类汇总，调整数据的展现格式</p>
<h3 id="插入切片器"><a class="markdownIt-Anchor" href="#插入切片器"></a> 插入切片器</h3>
<p>在excel顶部菜单中，找到分析→插入切片器→季度→确定。<br />
以上，数据透视表能帮我们统计数据源，切片器能帮助形成交互式报表，再通过公式提取数据生成图表。当使用切片器时，便生成了交互式图表。至此，excel做数据分析图的数据部分便做好了。</p>
<h3 id="条形图"><a class="markdownIt-Anchor" href="#条形图"></a> 条形图</h3>
<p>引用数据，用Large函数和Index+Match组合查找函数，在利润中提取排名前五和倒五的值和对应的省份，生成条形图。再选中数据→插入→查看所有图表→所有图表→条形图→簇状条形图，插入条形图。<br />
<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel4.jpg" alt="img" /></p>
<h3 id="瀑布图"><a class="markdownIt-Anchor" href="#瀑布图"></a> 瀑布图</h3>
<p>引用数据，用Sumif函数对各地区的利润求和，选中数据→插入→查看所有图表→所有图表→瀑布图→确定→双击总计的柱子→鼠标右键→设置为汇总，生成瀑布图。瀑布图适用于展示数据之间的演变过程，能直观看出销售额从0变化到总计的一个过程。<br />
<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel5.jpg" alt="img" /></p>
<h3 id="树状图"><a class="markdownIt-Anchor" href="#树状图"></a> 树状图</h3>
<p>用Vlookup函数查找利润，选中数据→插入→查看所有图表→所有图表→树状图→确定，生成树状图。树状图能快速了解比重情况及各区域分布详情。7、制作图表——折线图引用数据，用Sumif函数对各月的销售额求和，用Average函数求年销售额平均值。选中数据→插入→查看所有图表→所有图表→折线图→确定，生成折线图。折线图能分析每月销售额的变化情况，通过添加平均线，能帮助分析为什么这个月销售额高，为什么那个月销售额低。</p>
<h2 id="数据分析的几点硬核经验"><a class="markdownIt-Anchor" href="#数据分析的几点硬核经验"></a> 数据分析的几点硬核经验</h2>
<ol>
<li>务必提升数据采集的效率</li>
</ol>
<p>excel重处理而弱采集，尤其在大体量的公司，跨部门收集、汇总四面八方的数据，很崩溃特别是经常需要大面积采集数据，所以数据收集经常能将自己弄奔溃。</p>
<p>所以找了一些方便我们处理的表单工具（简道云、麦客、金数据、氚云等），一圈试下来，觉得钉钉+简道云可以搭配使用，数据收集效率还是很可观地。</p>
<p>数据采集还涉及线上数据爬取，但这方面我了解不多，就不班门弄斧了。有意者可以参考这篇回答：<br />
<a href="https://www.zhihu.com/question/20899988/answer/24923424">如何入门 Python 爬虫？</a><br />
<img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel6.jpg" alt="img" /><br />
2. 业务知识大于工具选择</p>
<p>所有数据分析师都会告诉后来人“业务知识很重要”，因为大家在踩了坑之后才恍然大悟分析中遇到的很多难题问题都源于对业务的不了解。</p>
<p>例如，同样是对客户进行分析，互联网电商的客户与保险客户具有明显区别，前者重视来源，活跃度，购买率，流失率，后者关注渠道，报价，理赔风险，投诉。业务知识包括这种大方向的行业知识，也包括公司内部特殊情况，了解得越详细可以避免绕很多弯路。</p>
<p>例如，有些行为是内部人员参与造成的数据异常要提前做处理，有些业务开展是带地区特性的，分析时候要区分对待等。</p>
<p>而真正做分析时候，你会发现市面上有太多的分析工具，需要掌握的实在是太多了，其实不必纠结于此，依据个人能力，配合当前的数据分析环境，适用的工具自然会被选出。</p>
<p>数据分析过来人都会说80％的时间都在做数据处理工作其中又有80%是做数据的清洗，所以选择好并学习好相关的数据分析的工具是重中之重特别是excel一定得认真学习，重点对待。</p>
<h2 id="日常积累"><a class="markdownIt-Anchor" href="#日常积累"></a> 日常积累</h2>
<p>就如同前面所说，书本知识让我们能够对Excel的基本知识有一个框架性的认识，如过真的想成长为Excel领域的专家，还是需要日常不断地加深和巩固。</p>
<p>针对加深和巩固，一方面是需要多用，把已掌握知识从会用变更熟练。二是不断汲取新知识，这个可以通过多留意一些论坛或者学习平台，<strong>“三人行，必有我师”</strong>，从与人交流中，能够看到很多自己并不了解的Excel用法。</p>
<p><img src="/images/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BExcel2.jpg" alt="img" /></p>
<p>这里，给大家推荐2个不错的论坛，以及收集的Excel全函数表。</p>
<p><a href="https://www.excelhome.net/">Excelhome</a><br />
<a href="http://www.excelpx.com/">Excel精英论坛</a><br />
<a href="https://www.ocexa.cn:10003/d/s/734967737417211928/IxagvuBhy9eusNurwcIeusd2ypJ4xiNw-GSBgj1whMwo_">Excel函数表</a></p>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>Excel</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>Excel</tag>
        <tag>数据可视化</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础理解</title>
    <url>/2023/02/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="数据挖掘是什么"><a class="markdownIt-Anchor" href="#数据挖掘是什么"></a> 数据挖掘是什么</h2>
<p>数据挖掘也叫机器学习，是由人工智能之父<strong>艾伦.图灵</strong>提出，其最大的成就就是图灵测试。简单理解为就是一个人和一个机器跟你去聊天，你不知道对方是人还是机器，如果经过聊天后，你分辨不出谁是人谁是机器则说明这个机器通过了图灵测试。</p>
<p><strong>机器学习</strong>是实现<strong>人工智能</strong>的一种技术手段，官方给出的人工智能诠释为：“机器学习就是从【数据】中自动分析获得【规律（模型）】，并利用规律对未知数据进行【预测】”。在通俗点讲就是“机器学习”开始是一位普通的厨师，通过不断的锻炼和菜谱（算法模型），将样本数据（食材）变为一道道美食，最终成为一位米其林大厨。</p>
<h2 id="数据挖掘基础"><a class="markdownIt-Anchor" href="#数据挖掘基础"></a> 数据挖掘基础</h2>
<ul>
<li>需要明确的几点：<br />
1.机器学习最终进行预测出来的结果其实都是通过相关的算法计算出来的结果！所以说在机器学习中算法是核心，数据是计算的基础。<br />
2.找准定位：大部分复杂模型的算法设计都是算法工程师（博士，硕士）在做，而我们只需要：
<ul>
<li>学会分析问题，使用机器学习相关算法完成对应的需求</li>
<li>掌握算法的基本思想，学会对不同问题选择对应的算法去解决</li>
<li>学会利用框架和库解决问题</li>
</ul>
</li>
</ul>
<h3 id="数据挖掘算法分类"><a class="markdownIt-Anchor" href="#数据挖掘算法分类"></a> 数据挖掘算法分类</h3>
<ul>
<li>分类和回归问题
<ul>
<li>分类算法基于的是【标签数据】为【离散型】数据<br />
回归算法基于的是【标签数据】为【连续型】数据</li>
<li>结论：在社会中产生的数据必然是离散型或者是连续型的数据，那么企业针对数据所产生的需求也无非是分类问题或者回归问题。</li>
</ul>
</li>
<li>分类问题应用：
<ul>
<li>人脸识别图像处理</li>
<li>文本分类</li>
<li>银行分类客户贷款风险</li>
</ul>
</li>
<li>回归问题应用：
<ul>
<li>股票</li>
<li>房价预测</li>
</ul>
</li>
</ul>
<h3 id="数据挖掘开发流程"><a class="markdownIt-Anchor" href="#数据挖掘开发流程"></a> 数据挖掘开发流程</h3>
<ul>
<li>1.数据采集
<ul>
<li>公司内部产生的数据</li>
<li>和其他公司合作获取的数据</li>
<li>购买的数据</li>
</ul>
</li>
<li>2.分析数据所对应要解决需求或者问题是什么？根据目标数据推断问题属于回归还是分类！</li>
<li>3.数据的基本处理
<ul>
<li>数据清洗</li>
<li>合并</li>
<li>级联等</li>
</ul>
</li>
<li>4.特征工程：对特征进行处理
<ul>
<li>特征抽取</li>
<li>特征预处理</li>
<li>降维等</li>
</ul>
</li>
<li>5.选择合适的模型，然后对其进行训练</li>
<li>6.模型的评估</li>
<li>7.上线使用</li>
</ul>
<h2 id="数据挖掘中的数据类型"><a class="markdownIt-Anchor" href="#数据挖掘中的数据类型"></a> 数据挖掘中的数据类型</h2>
<ul>
<li>机器学习中的数据类型分为：<br />
1.离散型数据：
<ul>
<li>取值范围是有限个值或者一个数列构成的，表示分类情况，如：企业数量 产品数量等</li>
<li>离散变量则是通过计数方式取得的，即是对所要统计的对象进行计数，增长量非固定的，如：一个地区的企业数目可以是今年只有一家，而第二年开了十家；一个企业的职工人数今年只有10人，第二年一次招聘了20人等。<br />
2.连续型数据：</li>
<li>连续变量是一直叠加上去的，增长量可以划分为固定的单位，即：1,2,3…… 例如：一个人的身高，他首先长到1.51，然后才能长到1.52，1.53……。</li>
<li>取值范围是一个区间，它可以在该区间中连续取值，即连续型变量可以取到区间中的任意值，并且有度量单位。例如：身高、年龄、体重、金额<br />
注意：</li>
<li>连续型数据的增长是有规律的,离散型数据的增长是没有规律的。</li>
<li>连续性数据是区间可分的，而离散型数据是区间不可分的。</li>
</ul>
</li>
</ul>
<h3 id="数据挖掘数据集划分"><a class="markdownIt-Anchor" href="#数据挖掘数据集划分"></a> 数据挖掘数据集划分</h3>
<p>是从数据中自动分析获得规律，并利用规律对未知数据进行预测。换句话说，我们的模型一定是要经过样本数据对其进行训练，才可以对未知数据进行预测的。<br />
问：我们得到数据后，是否将数据全部用来训练模型呢？<br />
当然不是的啦！因为我们如果模型（数据的规律）都是从数据中得来的，那么该模型的性能评估如何进行呢？还是基于对原先的数据进行预测吗？可想不是的，如果模型对原先的数据进行预测，由于模型（数据的规律）本来就是从该数据中获取的，所以预测的精度几乎会是百分之百。所以想要评估模型的好坏，需要使用一组新数据对模型进行评估。<br />
因此我们需要将原先的样本数据拆分成两部分：<br />
训练集：训练模型<br />
测试集：评估模型<br />
不同类型的模型对应的评估方式是不一样的</p>
<ul>
<li>数据集划分的API<br />
from sklearn.model_selection import train_test_split<br />
train_test_split(x,y,test_size,random_state)参数介绍：<br />
x：特征<br />
y：目标<br />
test_size：测试集的比例<br />
random_state：打乱的随机种子<br />
返回值：训练特征，测试特征，训练目标，测试目标</li>
</ul>
<h2 id="特征工程"><a class="markdownIt-Anchor" href="#特征工程"></a> 特征工程</h2>
<ul>
<li>
<p>为什么需要特征工程<br />
样本数据中的特征有可能会存在缺失值，重复值，异常值等等，那么我们是需要对特征中的相关的噪点数据进行处理的，那么处理的目的就是为了营造出一个更纯净的样本集（数据集越纯净则越便于让模型总结出数据集中潜在的规律），让模型基于这组数据可以有更好的预测能力。当然特征工程不是单单只是处理上述操作！</p>
</li>
<li>
<p>什么是特征工程<br />
特征工程是将原始数据转换为更好的能代表模型能够处理数据的潜在问题对应特征的过程，从而提高对未知数据预测的准确性。所以特征工程就是对特征的相关处理！<br />
比如AlphaGo学习的数据中既有棋谱，又有食谱还有歌词，那么一些干扰的数据绝对会影响AlphaGo的学习。</p>
</li>
<li>
<p>特征工程的意义<br />
直接影响模型预测的结果</p>
</li>
<li>
<p>如何实现特征工程<br />
工具：sk-learn</p>
</li>
<li>
<p>sklean介绍</p>
<ul>
<li>是python语言中的机器学习工具，包含了很多知名的机器学习算法的实现，其文档完善，容易上手。<br />
功能：<br />
分类模型<br />
回归模型<br />
聚类模型<br />
特征工程</li>
</ul>
</li>
</ul>
<h3 id="特征抽取"><a class="markdownIt-Anchor" href="#特征抽取"></a> 特征抽取</h3>
<p>目的：<br />
我们所采集到样本中的特征数据往往很多时候为字符串或者其他类型的数据，我们知道电脑只可以识别二进制数值型的数据，如果把字符串给电脑，电脑是看不懂的。机器学习学习的数据如果不是数值型的数据，它是识别不了的。</p>
<p>字符串类型的数据我们也可以成为分类变量</p>
<ul>
<li>
<p>无序分类变量</p>
<ul>
<li>说明事物类别的一个名称，如性别有男女两种，二者无大小之分，无顺序之分，还有如血型、民族等</li>
</ul>
</li>
<li>
<p>有序分类变量</p>
<ul>
<li>也是说明事物类型的一个名称，但是有次序之分，例如：满意度分为满意 一般 不满意，三者是有顺序的，you大小之分</li>
</ul>
</li>
</ul>
<p>特征值化：将非数值型的特征转换成数值型的特征</p>
<p>效果演示：<br />
将字符串转换成数字</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">vector = CountVectorizer()</span><br><span class="line">res = vector.fit_transform([<span class="string">&#x27;lift is short,i love python&#x27;</span>,<span class="string">&#x27;lift is too long,i hate python&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(res.toarray())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出：[[0 1 1 0 1 1 1 0]<br />
[1 1 1 1 0 1 0 1]]</p>
<p>演示后得出：<br />
特征抽取对文本等数据进行特征值化。特征值化是为了让机器更好的理解数据。</p>
<h4 id="字典特征抽取onehot编码"><a class="markdownIt-Anchor" href="#字典特征抽取onehot编码"></a> 字典特征抽取——OneHot编码</h4>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%A7%A31.png" alt="img" /></p>
<ul>
<li>为什么需要onehot编码呢？
<ul>
<li>特征抽取主要目的就是对非数值型的数据进行特征值化！如果现在需要对下图中的human和alien进行手动特征值化Alien为4，human<br />
<img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%A7%A32.png" alt="img" /></li>
</ul>
</li>
<li>对其进行One-Hot编码后：<br />
<img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%A7%A33.png" alt="img" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#基于pandas实现one-hot编码</span></span><br><span class="line"><span class="comment">#    pd.get_dummies(df[&#x27;col&#x27;])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>总结：</p>
<ul>
<li>理想情况下，无序分类变量的特征使用one-hot编码进行特征值化，有序分类变量的特征用map映射。</li>
</ul>
<h4 id="文本特征抽取"><a class="markdownIt-Anchor" href="#文本特征抽取"></a> 文本特征抽取</h4>
<ul>
<li>作用：对文本数据进行特征值化</li>
<li>API:from sklearn.feature_extraction.text import CountVectorizer</li>
<li>fit_transform(X):X为文本或者包含文本字符串的可迭代对象，返回sparse矩阵</li>
<li>inverse_transform(X)：X为array数组或者sparse矩阵，返回转换之前的格式数据</li>
<li>get_feature_names()</li>
<li>toarray()：将sparse矩阵换成数组</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line">alist = [</span><br><span class="line"></span><br><span class="line">         <span class="string">&#x27;left is is short,i love python&#x27;</span>,</span><br><span class="line"></span><br><span class="line">         <span class="string">&#x27;left is too long,i hate python&#x27;</span></span><br><span class="line"></span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">tool = CountVectorizer()</span><br><span class="line"></span><br><span class="line">ret = tool.fit_transform(alist)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tool.get_feature_names())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(ret.toarray())</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;hate&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;long&#x27;</span>, <span class="string">&#x27;love&#x27;</span>, <span class="string">&#x27;python&#x27;</span>, <span class="string">&#x27;short&#x27;</span>, <span class="string">&#x27;too&#x27;</span>]</span><br><span class="line">[[<span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p>上述文本抽取一般是给英文文章使用的，如果是中文文章，就要使用到“jieba分词”插件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#jieba分词安装</span></span><br><span class="line">pip install jieba</span><br><span class="line"></span><br><span class="line"><span class="comment">#jieba分词的基本使用</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">s = <span class="string">&#x27;分词作用到文本特征抽取的综合使用&#x27;</span></span><br><span class="line">ret = <span class="built_in">list</span>(jieba.cut(s))</span><br><span class="line"><span class="built_in">print</span>(ret)</span><br><span class="line"></span><br><span class="line"><span class="comment">#分词作用到文本特征抽取的综合使用</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">alist = [</span><br><span class="line">         <span class="string">&#x27;对有标点符号的中文文本进行特征抽取&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;因为在自然语言处理中，我们是需要将一段中文文本中相关的词语&#x27;</span></span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">jieba_alist = []</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> alist:</span><br><span class="line">    ret = <span class="built_in">list</span>(jieba.cut(item))</span><br><span class="line">    s_ret = <span class="string">&#x27; &#x27;</span>.join(ret)<span class="comment">#ret==&gt;[你好，我也好]  &#x27; &#x27;.join(list)==&gt;&quot;你好 我也好&quot;</span></span><br><span class="line">    jieba_alist.append(s_ret)</span><br><span class="line">    </span><br><span class="line">tool = CountVectorizer()</span><br><span class="line">ret = tool.fit_transform(jieba_alist)</span><br><span class="line"><span class="built_in">print</span>(tool.get_feature_names())</span><br><span class="line"><span class="built_in">print</span>(ret.toarray())</span><br></pre></td></tr></table></figure>
<h3 id="特征的预处理"><a class="markdownIt-Anchor" href="#特征的预处理"></a> 特征的预处理</h3>
<p>对数值性数据一般进行<strong>无量纲化</strong>处理</p>
<ul>
<li>
<p>无量纲化：</p>
<ul>
<li>在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一规格，或不同分布的数据转换到某个特定分布的需求这种需求统称为将数据“无量纲化”。</li>
<li>譬如梯度和矩阵为核心的算法中，譬如逻辑回归，支持向量机，神经 网络，无量纲化可以加快求解速度;</li>
<li>而在距离类模型，譬如K近邻，K-Means聚类中，无量纲化可以帮我们提升模型精度，避免某一个取值范围特别大的特征对距离计算造成影响。</li>
<li>一个特例是决策树和树的集成算法们，对决策 树我们不需要无量纲化，决策树可以把任意数据都处理得很好。<br />
那么预处理就是用来实现无量纲化的方式。<br />
含义：特征抽取后我们就可以获取对应的数值型的样本数据啦，然后就可以进行数据处理了。<br />
概念：通过特定的统计方法（数学方法），将数据转换成算法要求的数据<br />
方式：
<ul>
<li>归一化</li>
<li>标准化</li>
</ul>
</li>
</ul>
</li>
<li>
<p>归一化的实现：</p>
<ul>
<li>API:from sklearn.preprocessing import MinMaxScaler</li>
<li>参数：feature_range表示缩放范围，通常使用(0,1)</li>
<li>作用：使得某一个特征对最终结果不会造成很大的影响</li>
<li>问题：如果数据中存在的异常值比较多，会对结果造成什么样的影响？
<ul>
<li>结合着归一化计算的公式可知，异常值对原始特征中的最大值和最小值的影响很大，因此也会影响对归一化之后的值。这个也是归一化的一个弊端，无法很好的处理异常值。</li>
</ul>
</li>
<li>归一化总结：
<ul>
<li>在特定场景下最大值和最小值是变化的，另外最大最小值很容易受到异常值的影响，所以这种归一化的方式具有一定的局限性。因此引出了一种更好的方式叫做：标准化！！！</li>
</ul>
</li>
</ul>
</li>
<li>
<p>标准化的处理</p>
<ul>
<li>当数据按均值中心化后，再按标准差缩放，数据就会服从为均值为0，方差为1的正态分布(即标准正态分 布)，而这个过程，就叫做数据标准化(Standardization，又称Z-score normalization)。</li>
<li>API
<ul>
<li>处理后，每列所有的数据都聚集在均值为0，标准差为1范围附近</li>
<li>标准化API:from sklearn.preprocessing import StandardScaler
<ul>
<li>fit_transform(X):对X进行标准化</li>
<li>mean_：均值</li>
<li>var_:方差</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>归一化和标准化总结：</p>
<ul>
<li>对于归一化来说，如果出现了异常值则会响应特征的最大最小值，那么最终结果会受到比较大影响</li>
<li>对于标准化来说，如果出现异常点，由于具有一定的数据量，少量的异常点对于平均值的影响并不大，从而标准差改变比较少。</li>
</ul>
</li>
<li>
<p>StandardScaler和MinMaxScaler选哪个?</p>
<ul>
<li>看情况。大多数机器学习算法中，会选择StandardScaler来进行特征缩放，因为MinMaxScaler对异常值非常敏感。在PCA，聚类，逻辑回归，支持向量机，神经网络这些算法中，StandardScaler往往是最好的选择。 MinMaxScaler在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时使用广泛，比如数字图像处理中量化像素强度时，都会使用MinMaxScaler将数据压缩于[0,1]区间之中。</li>
<li>建议先试试看StandardScaler，效果不好换MinMaxScaler。</li>
</ul>
</li>
</ul>
<h4 id="特征选择"><a class="markdownIt-Anchor" href="#特征选择"></a> 特征选择</h4>
<p>从特征中选择出有意义对模型有帮助的特征作为最终的机器学习输入的数据！</p>
<ul>
<li>
<p>切记：</p>
<ul>
<li>在做特征选择之前，有三件非常重要的事:跟数据提供者联系，跟数据提供者沟通，跟数据提供者开会。</li>
<li>一定要抓住给你提供数据的人，尤其是理解业务和数据含义的人，跟他们聊一段时间。技术能够让模型起飞，前提是你和业务人员一样理解数据。所以特征选择的第一步，其实是根据我们的目标，用业务常识来选择特征。</li>
</ul>
</li>
<li>
<p>特征选择的原因：</p>
<ul>
<li>冗余：部分特征的相关度高，容易消耗计算机的性能
<ul>
<li>在房价预测中，有高度和楼层这两个特征</li>
</ul>
</li>
<li>噪点：部分特征对预测结果有偏执影响
<ul>
<li>在房价预测中，其中有户主血型这个特征</li>
</ul>
</li>
</ul>
</li>
<li>
<p>特征选择的实现：</p>
<ul>
<li>人为对不相关的特征进行主观舍弃</li>
<li>当然了，在真正的数据应用领域，比如金融，医疗，电商，我们的数据特征非常多，这样明显，那如果遇见极端情况，我们无法依赖对业务的理解来选择特征，该怎么办呢?
<ul>
<li>在已有特征和对应预测结果的基础上，使用相关的工具过滤掉一些无用或权重较低的特征
<ul>
<li>工具：
<ul>
<li>Filter（过滤式）【主要讲解】</li>
<li>PCA降维</li>
<li>相关性分析</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Filter过滤式（方差过滤）：</p>
<ul>
<li>原理：这是通过特征本身的方差来筛选特征的类。比如一个特征本身的方差很小，就表示样本在这个特征上基本没有差异，可能特征中的大多数值都一样，甚至整个特征的取值都相同，那这个特征对于样本区分没有什么作用。所以无论接下来的特征工程要做什么，都要优先消除方差为0或者方差极低的特征。
<ul>
<li>比如：朝阳区的房价预测，其中样本有一列特征为温度，则要知道朝阳区包含在样本中的房子对应的气象温度几乎一致或者大同小异，则温度特征则对房价的区分是无意义的。</li>
</ul>
</li>
<li>API:from sklearn.feature_selection import VarianceThreshold</li>
<li>VarianceThreshold(threshold=x)threshold方差的值，删除所有方差低于x的特征，默认值为0表示保留所有方差为非0的特征</li>
<li>fit_transform(X)#:X为特征</li>
</ul>
</li>
<li>
<p>PCA降维（主成分分析）：是一种分析，简化数据集的技术，也是【矩阵分解算法】的核心算法</p>
<ul>
<li>降维的维度值的就是特征的种类。</li>
<li>思想：如何最好的对一个立体的物体用二维表示<br />
<img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%90%86%E8%A7%A34.png" alt="img" /></li>
<li>当然，第四张二维图片可以比较好的标识一个立体三维的水壶。但是也要清楚，用一个低纬度去表示高纬度的物体时，一定会造成一些信息的差异。可以让低纬度也可以能正确的表示高纬度的事物，或者信息差异最小。</li>
<li>目的：特征数量达到上百，上千的时候，考虑数据的优化。使数据维度压缩，尽可能降低源数据的维度（复杂度），损失少量信息。</li>
<li>作用：可以削减回归分析或者聚类分析中特征的数量</li>
<li>PCA语法
<ul>
<li>from sklearn.decomposition import PCA</li>
<li>pca = PCA(n_components=None)
<ul>
<li>n_components可以为小数（保留特征的百分比），整数（减少到的特征数量）</li>
</ul>
</li>
<li>pca.fit_transform(X)</li>
<li>相关系数进行特征选择</li>
<li>希望样本的特征之间相关性越低越好</li>
<li>pandas中有个一方法corr可以计算df每一列的相关系数</li>
</ul>
</li>
</ul>
</li>
<li>
<p>相关系数：（不重要）</p>
<ul>
<li>作用：用来衡量两组数据之间的相关性</li>
<li>相关系数的取值范围是：1，-1</li>
<li>相关系数越接近1：说明两组数据之间越呈现正相关，否则呈现负相关。</li>
<li>在相关系数实现的特征选择中，我们只考虑特征之间的相关性不具体考虑正负相关</li>
<li>规律：相关系数大于0.6就表示两组数据之间呈现高相关性</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>数据挖掘</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据挖掘</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归及回归算法</title>
    <url>/2023/04/18/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%8F%8A%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>咳咳。之前因为有考试及生活中的琐事要处理，所以隔了很久才发布了这篇博客</p>
<h1 id="回归算法的应用"><a class="markdownIt-Anchor" href="#回归算法的应用"></a> 回归算法的应用</h1>
<ul>
<li>对于回归问题和如何使用线性回归算法做出最基础的判断
<ul>
<li>回归问题一般目标值是连续性的值，而分类问题的目标值是离散型的值。</li>
</ul>
</li>
<li>回归处理能做的预测
<ul>
<li>预测房价</li>
<li>销售额的预测</li>
<li>设定贷款额度</li>
<li>总结：可以根据事物的相关特征预测出对应的结果值，重点就是预测的能力</li>
</ul>
</li>
<li>线性回归在生活中的映射（现实生活中就有线性回归）：生活案例【预测学生的期末成绩】：
<ul>
<li>期末成绩的制定：0.7*考试成绩+0.3平时成绩，则该例子中，特征值为考试成绩和平时成绩，目标值为总成绩。从此案例中大概可以感受到
<ul>
<li>回归算法预测出来的结果其实就是经过相关的算法计算出来的结果值！</li>
<li>每一个特征需要有一个权重的占比，这个权重的占比明确后，则就可以得到最终的计算结果，也就是获取了最终预测的结果了。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>问题：假如现在有一套房子，面积为76.8平米，那么这套房子应该卖多少钱呢？也就是如何预测该套房子的价钱呢？<br />
下图中散点的分布情况就是面积和价钱这两个值之间的关系，那么如果该关系可以用一个走势的直线来表示的话，那么是不是就可以通过这条走势的直线预测出新房子的价格呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#现在有一组售房的数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line">dic = &#123;</span><br><span class="line">    <span class="string">&#x27;面积&#x27;</span>:[<span class="number">55</span>,<span class="number">76</span>,<span class="number">80</span>,<span class="number">100</span>,<span class="number">120</span>,<span class="number">150</span>],</span><br><span class="line">    <span class="string">&#x27;售价&#x27;</span>:[<span class="number">110</span>,<span class="number">152</span>,<span class="number">160</span>,<span class="number">200</span>,<span class="number">240</span>,<span class="number">300</span>]</span><br><span class="line">&#125;</span><br><span class="line">df = DataFrame(data=dic)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</span><br><span class="line">mpl.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;FangSong&#x27;</span>] <span class="comment"># 指定默认字体</span></span><br><span class="line">mpl.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span> <span class="comment"># 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span></span><br><span class="line"></span><br><span class="line">plt.scatter(df[<span class="string">&#x27;面积&#x27;</span>],df[<span class="string">&#x27;售价&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;面积&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;售价&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;面积和价钱的分布图&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921.png" alt="img" /><br />
<img src="/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%922.png" alt="img" /></p>
<ul>
<li>散点的趋势：
<ul>
<li>在上图中使用了一条直线来表示了房子的价格和面子对应的分布趋势，那么该趋势找到后，就可以基于该趋势根据新房子的面积预测出新房子的价格。</li>
</ul>
</li>
<li>线性回归的作用：
<ul>
<li>就是找出特征和目标之间存在的某种趋势！！！在二维平面中，该种趋势可以用一条线段来表示。</li>
</ul>
</li>
<li>该趋势使用什么表示呢？—》线性方程：
<ul>
<li>在数学中，线性方程y = wx就可以表示一条唯一的直线。那么在上述售房数据中，面积和价格之间的关系（二倍的关系）其实就可以映射成
<ul>
<li>价格 = 2 * 面积 ==》y=2x，这个方程就是价格和面积的趋势！也就是说根据该方程就可以进行新房子价格的预测</li>
</ul>
</li>
<li>标准的线性方程式为：y = wx + b,w为斜率，b为截距。是否带上b，得具体情况具体分析。y=wx,如果x为0，则y必定为0，那就意味着趋势对应的直线必过坐标系的原点（0，0），如果带上b值，则直线不过原点。如果上有图的趋势直线过原点的话，趋势就会不准。加b的目的是为了使得趋势对应的直线更加具有通用性！！！
<ul>
<li>如果目标值有可能为0的话，就带上b，否则不带b。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%923.png" alt="img" /></p>
<p>在预测中，肯定也会出现一种情况，误差</p>
<ul>
<li>那我们如何处理误差呢？在处理误差之前，我们必须先要知道一个回归算法的特性：
<ul>
<li>回归算法是一个迭代算法。所谓的迭代就好比是系统版本的迭代，迭代后的系统要比迭代前的系统更好。
<ul>
<li>当开始训练线性回归模型的时候，是逐步的将样本数据带入模型对其进行训练的。</li>
<li>训练开始时先用部分的样本数据训练模型生成一组w和b，对应的直线和数据对应散点的误差比较大，通过不断的带入样本数据训练模型会逐步的迭代不好（误差较大）的w和b从而使得w和b的值更加的精准。</li>
</ul>
</li>
<li>官方解释：迭代是重复反馈过程的活动，其目的通常是为了逼近所需目标或结果。每一次对过程的重复称为一次“迭代”，而每一次迭代得到的结果会作为下一次迭代的初始值。</li>
<li>API
<ul>
<li>最小二乘（正规方程）：</li>
</ul>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p><img src="/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%924.png" alt="img" /></p>
<h1 id="回归模型的评价指标"><a class="markdownIt-Anchor" href="#回归模型的评价指标"></a> 回归模型的评价指标</h1>
<ul>
<li>回归类算法的模型评估一直都是回归算法中的一个难点，回归类与分类型算法的模型评估其实是相似的法则— —找真实标签和预测值的差异。只不过在分类型算法中，这个差异只有一种角度来评判，那就是是否预测到了正确的分类，而在我们的回归类算法中，我们有两种不同的角度来看待回归的效果:
<ul>
<li>第一，我们是否预测到了正确或者接近正确的数值（因为误差的存在）。</li>
<li>第二，我们是否拟合到了足够的信息。（是否模型预测的结果线性和样本真实的结果的线性更加吻合）
<ul>
<li>这两种角度，分别对应着不同的模型评估指标。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="是否预测到了正确的数值"><a class="markdownIt-Anchor" href="#是否预测到了正确的数值"></a> 是否预测到了正确的数值</h2>
<ul>
<li>回忆一下我们的RSS残差平方和，它的本质是我们的预测值与真实值之间的差异，也就是从一种角度来评估我们回归的效力，所以RSS既是我们的损失函数，也是我们回归类模型的模型评估指标之一。但是，RSS有着致命的缺点: 它是一个无界的和，可以无限地大或者无限的小。我们只知道，我们想要求解最小的RSS，从RSS的公式来看，它不能为负，所以 RSS越接近0越好，但我们没有一个概念，究竟多小才算好，多接近0才算好?为了应对这种状况，sklearn中使用RSS 的变体，均方误差MSE(mean squared error)来衡量我们的预测值和真实值的差异:</li>
<li>均方误差，本质是在RSS的基础上除以了样本总量，得到了每个样本量上的平均误差。有了平均误差，我们就可以将平均误差和我们的标签的取值范围（最大值和最小值）在一起比较，以此获得一个较为可靠的评估依据。（查看这个错误有多严重）。
<ul>
<li>因为标签的最大值和最小值可以表示标签的一个分部情况，那么将其最大值和最小值和平均误差比较就可以大概看出在每个样本上的误差或者错误有多严重。</li>
</ul>
</li>
</ul>
<h2 id="是否拟合了足够的信息"><a class="markdownIt-Anchor" href="#是否拟合了足够的信息"></a> 是否拟合了足够的信息</h2>
<ul>
<li>对于回归类算法而言，只探索数据预测是否准确是不足够的。除了数据本身的数值大小之外，我们还希望我们的模型能够捕捉到数据的”规律“，比如数据的分布规律（抛物线），单调性等等。而是否捕获到这些信息是无法使用MSE来衡量的。<br />
<img src="/images/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%925.png" alt="img" /></li>
<li>来看这张图，其中红色线是我们的真实标签，而蓝色线是我们模型预测的值。这是一种比较极端，但的确可能发生的情况。这张图像上，前半部分的拟合非常成功，看上去我们的真实标签和我们的预测结果几乎重合，但后半部分的拟合 却非常糟糕，模型向着与真实标签完全相反的方向去了。对于这样的一个拟合模型，如果我们使用MSE来对它进行判 断，它的MSE会很小，因为大部分样本其实都被完美拟合了，少数样本的真实值和预测值的巨大差异在被均分到每个 样本上之后，MSE就会很小。但这样的拟合结果必然不是一个好结果，因为一旦我的新样本是处于拟合曲线的后半段的，我的预测结果必然会有巨大的偏差，而这不是我们希望看到的。所以，我们希望找到新的指标，除了判断预测的 数值是否正确之外，还能够判断我们的模型是否拟合了足够多的，数值之外的信息。</li>
</ul>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>数据挖掘</category>
        <category>机器学习</category>
        <category>回归算法</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据挖掘</tag>
        <tag>机器学习</tag>
        <tag>回归算法</tag>
      </tags>
  </entry>
  <entry>
    <title>过拟合以及欠拟合的处理</title>
    <url>/2023/04/27/%E8%BF%87%E6%8B%9F%E5%90%88%E4%BB%A5%E5%8F%8A%E6%AC%A0%E6%8B%9F%E5%90%88/</url>
    <content><![CDATA[<h1 id="什么是欠拟合过拟合"><a class="markdownIt-Anchor" href="#什么是欠拟合过拟合"></a> 什么是欠拟合&amp;过拟合？</h1>
<ul>
<li>问题：训练好的模型在训练集上表现的预测效果很好，但是在测试集上却有很大的问题和误差，why，让我们看下以下两个案例？
<ul>
<li>案例1：
<ul>
<li>现在有一组天鹅的特征数据然后对模型进行训练，然后模型学习到的内容是有翅膀，嘴巴长的就是天鹅。然后使用模型进行预测，该模型可能会将所有符合这两个特征的动物都预测为天鹅，则肯定会有误差的，因为鹦鹉，秃鹫都符合有翅膀和嘴巴长的特征。
<ul>
<li>原因：模型学习到的天鹅的特征太少了，导致区分标准太粗糙，不能准确的识别出天鹅。</li>
</ul>
</li>
</ul>
</li>
<li>案例2：
<ul>
<li>更新了样本的特征数据了，增加了一些特征，然后训练模型。模型这次学习到的内容是，有翅膀、嘴巴长、白色、体型像2、脖子长且有弯度的就是天鹅。然后开始使用模型进行预测，现在一组测试数据为鹦鹉，因为鹦鹉的体型小，脖子短不符合天鹅的特征，则预测结果为不是天鹅。然后又有一组特征为黑天鹅，结果因为颜色不是白色，预测成了黑天鹅。
<ul>
<li>原因：现在模型学习到的特征已经基本可以区分天鹅和其他动物了。但是学习到的特征中有一项是羽毛是白色，那么就会导致将黑天鹅无法识别出来。也就是机器学习到的特征太依赖或者太符合训练数据了。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>欠拟合：案例1中的场景就可以表示欠拟合
<ul>
<li>欠拟合是指模型在训练集、验证集和测试集上均表现不佳的情况；（模型过于简单）</li>
</ul>
</li>
<li>过拟合：案例2中的场景就可以表示过拟合
<ul>
<li>过拟合是指模型在训练集上表现很好，到了验证和测试阶段就很差，即模型的泛化能力很差。（模型过于复杂）</li>
</ul>
</li>
</ul>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%881.png" alt="img" /></p>
<h1 id="关于欠拟合和过拟合的解决"><a class="markdownIt-Anchor" href="#关于欠拟合和过拟合的解决"></a> 关于欠拟合和过拟合的解决</h1>
<ul>
<li>欠拟合：
<ul>
<li>原因：<strong>模型学习到样本的特征太少。</strong>
<ul>
<li>解决：增加样本的特征数量（多项式回归）</li>
</ul>
</li>
</ul>
</li>
<li>过拟合：
<ul>
<li>原因：<strong>原始特征过多，存在一些嘈杂特征。</strong>
<ul>
<li>解决：
<ul>
<li>进行特征选择，消除关联性大的特征（很难做）</li>
<li>正则化之岭回归（掌握）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>模型的复杂度–》回归出直线or曲线：
<ul>
<li>我们的回归模型最终回归出的一定是直线吗(y=wx+b)？有没有可能是曲线（非线性）呢（y=wx**2+b）？
<ul>
<li>我们都知道回归模型算法就是在寻找特征值和目标值之间存在的某种关系，那么这种关系越复杂则表示训练出的模型的复杂度越高，反之越低。</li>
<li>模型的复杂度是由特征和目标之间的关系导致的！特征和目标之间的关系不仅仅是线性关系！</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="欠拟合的处理多项式回归"><a class="markdownIt-Anchor" href="#欠拟合的处理多项式回归"></a> 欠拟合的处理：多项式回归</h2>
<ul>
<li>为了解决欠拟合的情 经常要提高线性的次数（高次多项式）建立模型拟合曲线，次数过高会导致过拟合，次数不够会欠拟合。
<ul>
<li>y = w*x + b   一次多项式函数</li>
<li>y = w1<em>x^2 + w2</em>x + b  二次多项式函数</li>
<li>y = w1<em>x^3 + w2</em>x^2 + w3*x + b  三次多项式函数</li>
<li>。。。
<ul>
<li>高次多项式函数的表示为曲线。<br />
实例效果：</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">3</span></span><br><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%882.png" alt="img" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">2</span>*x**<span class="number">2</span> + <span class="number">3</span></span><br><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%883.png" alt="img" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">20</span>)</span><br><span class="line">y = <span class="number">2</span> * x + <span class="number">2</span>*x**<span class="number">2</span> + x**<span class="number">3</span> + <span class="number">3</span></span><br><span class="line">plt.plot(x,y)</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%884.png" alt="img" /><br />
相对于线性回归模型y=wx+b只能解决线性(回归出的为直线)问题，多项式回归能够解决非线性回归（回归出的为曲线）问题。<br />
拿最简单的线性模型来说，其数学表达式可以表示为：y=wx+b，它表示的是一条直线，而多项式回归则可以表示成：y=w1x∧2+w2x+b,它表示的是二次曲线，<h4>实际上，多项式回归可以看成特殊的线性模型</h4>，即把x∧2看成一个特征，把x看成另一个特征，这样就可以表示成y=w1z+w2x+b,其中z=x∧2,这样多项式回归实际上就变成线性回归了。<br />
其中的y=w1x∧2+w2x+b就是所谓的二次多项式:aX∧2+bX+c(a≠0).<br />
当然还可以将y=wx+b转为更高次的多项式。是否需要转成更高次的多项式取决于我们想要拟合样本的程度了，更高次的多项式可以更好的拟合我们的样本数据，但是也不是一定的，很可能会造成过拟合。</p>
<ul>
<li>建立二次多项式线性回归模型进行预测
<ul>
<li>根据二次多项式公式可知，需要给原始特征添加更高次的特征数据x^2.
<ul>
<li>y=w1x∧2+w2x+b</li>
</ul>
</li>
<li>如何给样本添加高次的特征数据呢？
<ul>
<li>使用sklearn.preprocessing.PolynomialFeatures来进行更高次特征的构造
<ul>
<li>它是使用多项式的方法来进行的，如果有a，b两个特征，那么它的2次多项式为（1,a,b,a^2,ab, b^2）</li>
<li>PolynomialFeatures有三个参数
<ul>
<li>degree：控制多项式的度</li>
<li>interaction_only： 默认为False，如果指定为True，上面的二次项中没有a<sup>2和b</sup>2。</li>
<li>include_bias：默认为True。如果为False的话，那么就不会有上面的1那一项</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建一个随机数表</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">feature = pd.DataFrame(data=np.random.randint(<span class="number">0</span>,<span class="number">10</span>,size=(<span class="number">5</span>,<span class="number">2</span>)))</span><br><span class="line"><span class="comment">#给原始一个维度的特征增加高次项特征</span></span><br><span class="line">tool = PolynomialFeatures(degree=<span class="number">2</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line">ret = tool.fit_transform(feature)</span><br><span class="line">ret</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%885.png" alt="img" /></p>
<h3 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h3>
<p>下面模拟根据蛋糕的直径大小预测蛋糕价格。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本的训练数据，特征和目标值</span></span><br><span class="line">x_train = [[<span class="number">6</span>], [<span class="number">8</span>], [<span class="number">10</span>], [<span class="number">14</span>], [<span class="number">18</span>]] <span class="comment">#大小</span></span><br><span class="line">y_train = [[<span class="number">7</span>], [<span class="number">9</span>], [<span class="number">13</span>], [<span class="number">17.5</span>], [<span class="number">18</span>]]<span class="comment">#价格</span></span><br><span class="line">plt.scatter(x_train,y_train) <span class="comment">#原始样本的分布情况</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%886.png" alt="img" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#建模</span></span><br><span class="line">linner = LinearRegression()</span><br><span class="line">linner.fit(x_train,y_train) <span class="comment">#fit参数的X为啥是大写？大写的X是要求我们传入模型的特征必须是二维的（矩阵）</span></span><br><span class="line"><span class="comment">#模型预测的结果</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,r2_score</span><br><span class="line">y_pred = linner.predict(x_train)</span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line">plt.scatter(x_train,y_train)</span><br><span class="line">plt.plot(x_train,y_pred)</span><br><span class="line"></span><br><span class="line">mean_squared_error(y_train,y_pred),r2_score(y_train,y_pred)</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%887.png" alt="img" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#给数据增加2次项特征</span></span><br><span class="line">tool = PolynomialFeatures(degree=<span class="number">2</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line">d_2_x_train = tool.fit_transform(x_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#建模</span></span><br><span class="line">linner = LinearRegression()</span><br><span class="line">linner.fit(d_2_x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画图：无需理解</span></span><br><span class="line">plt.scatter(x_train,y_train)</span><br><span class="line">y_pred = linner.predict(d_2_x_train)</span><br><span class="line">plt.plot(x_train,y_pred)</span><br><span class="line"></span><br><span class="line">mean_squared_error(y_train,y_pred),r2_score(y_train,y_pred)</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%888.png" alt="img" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#给数据增加3次项特征</span></span><br><span class="line">tool = PolynomialFeatures(degree=<span class="number">3</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line">d_3_x_train = tool.fit_transform(x_train)</span><br><span class="line"><span class="comment">#建模</span></span><br><span class="line">linner = LinearRegression()</span><br><span class="line">linner.fit(d_3_x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画图：无需理解</span></span><br><span class="line">plt.scatter(x_train,y_train)</span><br><span class="line">y_pred = linner.predict(d_3_x_train)</span><br><span class="line">plt.plot(x_train,y_pred)</span><br><span class="line">mean_squared_error(y_train,y_pred),r2_score(y_train,y_pred)</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%889.png" alt="img" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#给数据增加4次项特征</span></span><br><span class="line">tool = PolynomialFeatures(degree=<span class="number">4</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line">d_4_x_train = tool.fit_transform(x_train)</span><br><span class="line"><span class="comment">#建模</span></span><br><span class="line">linner = LinearRegression()</span><br><span class="line">linner.fit(d_4_x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#画图：无需理解</span></span><br><span class="line">plt.scatter(x_train,y_train)</span><br><span class="line">y_pred = linner.predict(d_4_x_train)</span><br><span class="line">plt.plot(x_train,y_pred)</span><br><span class="line">mean_squared_error(y_train,y_pred),r2_score(y_train,y_pred)</span><br></pre></td></tr></table></figure>
<p><img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%8810.png" alt="img" /></p>
<ul>
<li><strong>使用增加高次项特征的策略查看是够可以适当解决房价预测模型的欠拟合症状</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#分别增加2、3、4次项新维度特征”degree = 2、3、4</span></span><br><span class="line">tool = PolynomialFeatures(degree=<span class="number">2</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line">d_2_feature = tool.fit_transform(feature)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据集切分</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(d_2_feature,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#建模</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型评估</span></span><br><span class="line">y_true_test = y_test</span><br><span class="line">y_pred_test = model.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型在测试集的表现结果:&#x27;</span>,MSE(y_true_test,y_pred_test),r2(y_true_test,y_pred_test))</span><br><span class="line"></span><br><span class="line">y_true_train = y_train</span><br><span class="line">y_pred_train = model.predict(x_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型在训练集的表现结果:&#x27;</span>,MSE(y_true_train,y_pred_train),r2(y_true_train,y_pred_train))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>下面是原始特征训练模型后，对应的评价结果</p>
<ul>
<li>(55.33406050090192, 0.6108181277767878) 测试集</li>
<li>(83.00347064630587, 0.575098424925357)  训练集</li>
</ul>
</li>
<li>
<p>增加了2次项特征后模型结果（最好）</p>
<ul>
<li>模型在测试集的表现结果: 30.830177954866645 0.7831616500066133</li>
<li>模型在训练集的表现结果: 60.80409444996298 0.6887388527057157</li>
</ul>
</li>
<li>
<p>增加了3次项特征后模型结果</p>
<ul>
<li>模型在测试集的表现结果: 33.96117658337967 0.7611403506994145</li>
<li>模型在训练集的表现结果: 49.49512455918217 0.7466303972598027</li>
</ul>
</li>
<li>
<p>增加了4次项特征后模型结果</p>
<ul>
<li>模型在测试集的表现结果: 135.47253896643744 0.04717897308601049</li>
<li>模型在训练集的表现结果: 80.61314731189564 0.5873346861541975</li>
</ul>
</li>
</ul>
<h2 id="过拟合处理正则化"><a class="markdownIt-Anchor" href="#过拟合处理正则化"></a> 过拟合处理:正则化</h2>
<p>什么是正则化？<br />
对损失函数加入一个惩罚项，使得模型由多解变为更倾向其中一个解(更加精准的预测)。</p>
<ul>
<li>正则化项
<ul>
<li>前面使用多项式回归，如果多项式最高次项比较大，模型就容易出现过拟合。正则化是一种常见的防止过拟合的方法，一般原理是在损失函数后面加上一个对参数(w)的约束项，这个约束项被叫做正则化项（regularizer）。在线性回归模型中，通常有两种不同的正则化项：
<ul>
<li>加上所有参数的绝对值之和，即L1范数，此时叫做Lasso回归</li>
<li>加上所有参数即L2范数(所有参数的平方和)，此时叫做岭回归</li>
</ul>
</li>
</ul>
</li>
<li>注意：
<ul>
<li>LinnerRegression是没有办法进行正则化的，所以该算法模型容易出现过拟合，并且无法解决。</li>
</ul>
</li>
</ul>
<h3 id="ridge岭回归具备l2正则化的线性回归模型"><a class="markdownIt-Anchor" href="#ridge岭回归具备l2正则化的线性回归模型"></a> Ridge岭回归:具备L2正则化的线性回归模型</h3>
<ul>
<li>岭回归也是一种用于回归的&quot;线性模型&quot;(因此它求解系数w也是使用最小二乘法实现的)但在岭回归中，对系数（w）的选择不仅要使得模型在训练数据上得到好的预测结果，而且还要给模型添加拟合的附加“约束”，就是希望系数w尽量小。</li>
<li>换句话说，就是所有w都应接近于0。直观上来看，这意味着每个特征对输出的影响应尽可能小（即斜率很小），同时模型仍可以给出很好的预测结果。</li>
<li>这种“约束”就是是所谓正则化（regularization）。正则化是指对模型做显式约束，以避免过拟合。</li>
<li>岭回归用到的这种被称为 L2 正则化。</li>
</ul>
<h3 id="示例-2"><a class="markdownIt-Anchor" href="#示例-2"></a> 示例</h3>
<p>岭回归在波士顿房价上的预测效果（如果要用该数据集，请先导入mglearn库）</p>
<p>pip install mglearn</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="comment">#提取样本数据</span></span><br><span class="line">feature,target = mglearn.datasets.load_extended_boston()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#切分数据集</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">2020</span>)</span><br><span class="line"><span class="comment">#建模</span></span><br><span class="line">linner = LinearRegression()</span><br><span class="line">linner.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#模型评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集的表现:&#x27;</span>,r2_score(y_test,linner.predict(x_test)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集的表现:&#x27;</span>,r2_score(y_train,linner.predict(x_train)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试集的表现: 0.6686661366503848</span></span><br><span class="line"><span class="comment">#训练集的表现: 0.9390074922223917</span></span><br><span class="line"><span class="comment">#发现模型在训练集表现的好，测试集表现的不好，说明模型出现了过拟合！！！</span></span><br></pre></td></tr></table></figure>
<ul>
<li>上述实验观测到的结果：
<ul>
<li>岭回归在训练集上的分数要低于线性回归，但在测试集上的分数更高。</li>
<li>由于岭回归的约束性更强，因此不容易出现过拟合；复杂度更小(系数w更小,更接近于0)的模型意味着在训练集上的性能更差，但泛化性（在更多未知数据集上的表现）能更好。由于我们只对泛化性能感兴趣，所以应该选择岭回归模型而不是线性回归模型。</li>
</ul>
</li>
<li>岭回归的超参数：alpha
<ul>
<li>增大 alpha 会使得系数更加趋向于 0，从而降低训练集性能，但可能会提高泛化性能；</li>
<li>减小 alpha 可以让系数受到的限制更小。</li>
<li>如何选择alpha的值：
<ul>
<li>选择正确alpha的值有助于模型学习正确的特征并有更好的泛化能力，因此交叉验证/学习曲线是帮助选择正确值的一种方法。</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge <span class="comment">#岭回归模型</span></span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"></span><br><span class="line"><span class="comment">#提取样本数据</span></span><br><span class="line">feature,target = mglearn.datasets.load_extended_boston()</span><br><span class="line"><span class="comment">#数据集切分</span></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">2020</span>)</span><br><span class="line"><span class="comment">#建模</span></span><br><span class="line">ridge = Ridge(alpha=<span class="number">0.01</span>) <span class="comment">#alpha的值也大则表示L2正则化处理过拟合的力度越大</span></span><br><span class="line"><span class="comment">#max_iter:模型最大的迭代次数（尽量稍微大点）</span></span><br><span class="line"></span><br><span class="line">ridge.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型的评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集的表现:&#x27;</span>,r2_score(y_test,ridge.predict(x_test)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集的表现:&#x27;</span>,r2_score(y_train,ridge.predict(x_train)))</span><br><span class="line"><span class="comment">#测试集的表现: 0.871776615421402</span></span><br><span class="line"><span class="comment">#训练集的表现: 0.9278219811981989</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="岭回归的应用场景"><a class="markdownIt-Anchor" href="#岭回归的应用场景"></a> 岭回归的应用场景</h3>
<ul>
<li>接下来我们来通过固定 alpha 值，但改变训练数据量来理解岭回归的正则化：
<ul>
<li>针对波士顿房价数据集，在不断增加训练样本的情况下分别对 LinearRegression 和 Ridge(alpha=1) 两个模型进行评估：<br />
<img src="/images/%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%8811.png" alt="img" /></li>
<li>由于岭回归是正则化的，因此它的训练集分数要整体低于线性回归的训练集分数。但岭回归的测试分数要更高，特别是对较小的子数据集。如果少于 400 个数据点，线性回归学不到任何内容。随着模型可用的数据越来越多，两个模型的性能都在提升，最终线性回归的性能追上了岭回归。</li>
<li>这里要记住的是，如果有足够多的训练数据，正则化变得不那么重要，并且岭回归和线性回归将具有相同的性能。</li>
</ul>
</li>
</ul>
<h3 id="lasso回归"><a class="markdownIt-Anchor" href="#lasso回归"></a> lasso回归</h3>
<ul>
<li>与岭回归相同，lasso回归也是约束系数w使其接近于 0,但用到的方法不同， lasso回归的约束使用的是L1正则化。</li>
<li>L1正则化的结果是，某些参数w会被缩减压缩到0。这说明某些特征被模型完全忽略。这可以看作是一种自动化的特征选择。</li>
<li>某些系数刚好为 0，这样的模型更容易解释，也可以呈现模型最重要的特征。<br />
我们继续将lasso回归呈现于波士顿房价的数据集上</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">import</span> mglearn</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">feature,target = mglearn.datasets.load_extended_boston()</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">model = Lasso(alpha=<span class="number">1</span>)</span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"><span class="comment">#模型评估</span></span><br><span class="line"><span class="comment">#测试集的表现: 0.22140772570108613</span></span><br><span class="line"><span class="comment">#训练集的表现: 0.23183091909679476</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(model.coef_ != <span class="number">0</span>).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment">#4</span></span><br></pre></td></tr></table></figure>
<p>可以发现，lasso回归的分数无论是在测试集还是训练集上都非常的差；这说明存在欠拟合，通过最后一行代码我们可以知道模型只用到了104个特征中的 4 个。<br />
模型超参数：</p>
<ul>
<li>alpha：与 岭回归类似，Lasso 也有一个正则化参数 alpha，可以控 制系数趋向于 0 的强度。在上一个例子中，我们用的是默认值 alpha=1.0。为了降低欠拟合，我们尝试减小 alpha。</li>
<li>这么做的同时，我们还需要增加 max_iter 的值（运行迭代的最大次数）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature,target = mglearn.datasets.load_extended_boston()</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">model = Lasso(alpha=<span class="number">0.1</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集的表现:&#x27;</span>,r2_score(y_test,model.predict(x_test)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集的表现:&#x27;</span>,r2_score(y_train,model.predict(x_train)))</span><br><span class="line">feature,target = mglearn.datasets.load_extended_boston()</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">​</span><br><span class="line"></span><br><span class="line">model = Lasso(alpha=<span class="number">0.1</span>,max_iter=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">​</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集的表现:&#x27;</span>,r2_score(y_test,model.predict(x_test)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集的表现:&#x27;</span>,r2_score(y_train,model.predict(x_train)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试集的表现: 0.7246460263073</span></span><br><span class="line"><span class="comment">#训练集的表现: 0.7338444027523867</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h2>
<p>在实践中，在两个模型中一般首选岭回归。但如果特征很多，你认为只有其中几个是重要 的，那么选择 Lasso 可能更好。同样，如果你想要一个容易解释的模型，Lasso 可以给出 更容易理解的模型，因为它只选择了一部分输入特征。</p>
<h1 id="模型的保存和加载"><a class="markdownIt-Anchor" href="#模型的保存和加载"></a> 模型的保存和加载</h1>
<ul>
<li>from sklearn.externals import joblib
<ul>
<li>joblib.dump(model,‘xxx.m’):保存</li>
<li>joblib.load(‘xxx.m’):加载</li>
</ul>
</li>
<li>import pickle
<ul>
<li>with open(’./123.pkl’,‘wb’) as fp:
<ul>
<li>pickle.dump(linner,fp)</li>
</ul>
</li>
<li>with open(’./123.pkl’,‘rb’) as fp:
<ul>
<li>linner = pickle.load(fp)</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">model = Lasso(alpha=<span class="number">0.001</span>)</span><br><span class="line">model.fit(x_train,y_train) <span class="comment">#训练好的模型</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./house_pirce.pkl&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    pickle.dump(model,fp) <span class="comment">#将训练好的模型model保存到fp表示的文件中</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载模型</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./house_pirce.pkl&#x27;</span>,<span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    model = pickle.load(fp)</span><br><span class="line">    model <span class="comment">#输出</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>数据挖掘</category>
        <category>机器学习</category>
        <category>拟合</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据挖掘</tag>
        <tag>机器学习</tag>
        <tag>拟合</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑斯蒂回归</title>
    <url>/2023/05/04/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h2 id="逻辑回归"><a class="markdownIt-Anchor" href="#逻辑回归"></a> 逻辑回归</h2>
<p>在之前的课程中我们已经学习接触过相关的回归模型了，我们知道回归模型是用来处理和预测连续型标签的算法。然而逻辑回归，是一种名为“回归”的线性分类器，其本质是由线性回归变化而来的，一种广泛使用于分类问题中的广义回归算法。要理解逻辑回归从何而来，得要先理解线性回归。线性回归是机器学习中最简单的的回归算法，它写作一个几乎人人熟悉的方程（为了更好理解本节后面的讲解到的sigmod函数，下面的回归函数用z来表示）:<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%921.png" alt="img" /></p>
<p>其中𝛉0被称为截距(intercept)， 𝛉1~𝛉n 被称为系数(coefficient)，这个表达式，其实就和我们中学时就无比熟悉的y=ax+b是同样的性质。我们可以使用矩阵来表示这个方程，其中x和𝛉都可以被看做是一个列矩阵，则有：<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%922.png" alt="img" /></p>
<p>通过上图函数z，线性回归使用输入的特征矩阵X来输出一组连续型的标签值y_pred，以完成各种预测连续型变量的任务 (比如预测产品销量，预测股价等等)。<br />
那如果我们的标签是离散型变量，尤其是，如果是满足0-1分布的离散型变量，我们要怎么办呢？<br />
可以使用逻辑回归这个回归模型，如果需要实现分类效果使用<strong>sigmod函数</strong>。<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%923.png" alt="img" /></p>
<ul>
<li>函数解释：
<ul>
<li>Sigmoid函数是一个S型的函数，当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0，它能够将任何实数（非0和1的标签数据）映射到(0,1)区间，使其可用于将任意值函数转换为更适合二分类的函数。 因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与我们之前学过的MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。区别在于，MinMaxScaler归一化之后，是可以取到0和1的(最大值归一化后就是1，最小值归一化后就是0)，但Sigmoid函数只是无限趋近于0和1。<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%924.png" alt="img" /></li>
</ul>
</li>
<li>简单来说:
<ul>
<li>逻辑回归就是使用simoid函数将线性方程返回的结果（线性回归模型预测出来的结果）压缩到0-1之间，从而实现的二分类的任务！</li>
<li>逻辑回归 == （线性回归+sigmiod函数）</li>
</ul>
</li>
<li>总之
<ol>
<li>逻辑回归就是线性回归+sigmoid函数，在逻辑回归里是有着求解权重系数w的需求。</li>
<li>sigmoid函数返回的结果就是样本分到某一个类别的概率。</li>
</ol>
</li>
</ul>
<h3 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h3>
<ul>
<li>在逻辑回归分类的时候，不管原始样本中的类别使用怎样的值或者文字表示，逻辑回归统一将其视为0类别和1类别。</li>
<li>因为逻辑回归也采用了寻找特征和目标之间的某种关系，则每个特征也是有权重的就是w，那么也会存在真实值和预测值之间的误差（损失函数），那么逻辑回归的损失函数和线性回归的损失函数是否一样呢？
<ul>
<li>由于逻辑回归是用于分类的，因此该损失函数和线性回归的损失函数是不一样的！逻辑回归采用的损失函数是：对数似然损失函数：</li>
</ul>
</li>
<li>注意：没有求解参数需求的模型是没有损失函数的，比如KNN，决策树。</li>
<li>损失函数被写作如下：
<ul>
<li>为什么使用-log函数为损失函数，损失函数的本质就是，如果我们预测对了，则没有损失，反之则损失需要变的很大，而-log函数在【0，1】之间正好符合这一点(-log可以放大损失)。
<ul>
<li>-log(h)表示分类到正例1的损失</li>
<li>-log(1-h)表示分类到反例0的损失</li>
</ul>
</li>
<li>损失函数表征预测值与真实值之间的差异程度，如果预测值与真实值越接近则损失函数应该越小<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%925.png" alt="img" /></li>
</ul>
</li>
</ul>
<p>解释：</p>
<ul>
<li>
<p>yilog(h)表示分类到真实标签的正例的损失，根据-log函数得知如果分类正确则损失值小，反之损失大</p>
</li>
<li>
<p>-(1-yi)log(1-h)表示分类到真实标签反例的损失，根据-log函数得知如果分类正确则损失小，反之损失大</p>
</li>
<li>
<p>那么两者相加就获得了逻辑回归模型总分类结果的损失!</p>
</li>
<li>
<p>将逻辑回归对应的预测结果带入损失函数：<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%926.png" alt="img" /></p>
<h3 id="梯度下降"><a class="markdownIt-Anchor" href="#梯度下降"></a> 梯度下降</h3>
<ul>
<li>逻辑回归的数学目的是求解能够让模型最优化，拟合程度最好的参数𝛉的值，即求解能够让损失函数J(𝛉)最小化的𝛉值。</li>
<li>梯度下降原理介绍：
<ul>
<li>假设现在有一个带两个特征并且没有截距的逻辑回归y(x1,x2)，两个特征所对应的参数分别为[𝛉1,𝛉2]。下面这个华丽的平面就是我们的损失函数 J(𝛉1,𝛉2)在以𝛉1,𝛉2和J为坐标轴的三维立体坐标系上的图像。现在，我们寻求的是损失函数的最小值,也就是图像的最低点。<br />
<img src="/images/逻辑斯蒂回归1.gif" alt="show" /></li>
</ul>
</li>
</ul>
</li>
<li>
<p>那我们怎么做呢?我在这个图像上随机放一个小球，当我松手，这个小球就会顺着这个华丽的平面滚落，直到滚到深蓝色的区域——损失函数的最低点。为了严格监控这个小球的行为，我让小球每次滚动的距离有限，不让他一次性滚到最低点，并且最多只允许它滚动100步，还要记下它每次滚动的方向，直到它滚到图像上的最低点。</p>
</li>
<li>
<p>可以看见，小球从高处滑落，在深蓝色的区域中来回震荡，最终停留在了图像凹陷处的某个点上。非常明显，我们可以观察到几个现象:</p>
<ul>
<li>首先，小球并不是一开始就直向着最低点去的，它先一口气冲到了蓝色区域边缘，后来又折回来，我们已经规定了小球是多次滚动，所以可见，小球每次滚动的方向都是不同的。</li>
<li>另外，小球在进入深蓝色区域后，并没有直接找到某个点，而是在深蓝色区域中来回震荡了数次才停下。停下来有两种可能:
<ul>
<li>1、 小球已经滚到了图像的最低点，所以停下了，</li>
<li>2、 由于我设定的步数限制，小球还没有找到最低点，但也只好在100步的时候停下了。也就是说，小球不一定滚到了图像的最低处。</li>
</ul>
</li>
<li>但无论如何，小球停下的就是我们在现有状况下可以获得的唯一点了。如果我们够幸运，这个点就是图像的最低点，那我们只要找到这个点的对应坐标(𝛉1，𝛉2，J)，就可以获取能够让损失函数最小的参数取值[𝛉1,𝛉2]了。如此，梯度下降的过程就已经完成。</li>
</ul>
</li>
<li>
<p>在这个过程中，小球其实就是一组组的坐标点(𝛉1，𝛉2，J);小球每次滚动的方向就是那一个坐标点的梯度向量的方向，因为每滚动一步，小球所在的位置都发生变化，坐标点和坐标点对应的梯度向量都发生了变化，所以每次滚动的方向也都不一样;人为设置的100次滚动限制，就是sklearn中逻辑回归的参数max_iter，代表着能走的最大步数.</p>
<ul>
<li>所以梯度下降，其实就是在众多[𝛉1,𝛉2]可能的值中遍历，一次次求解坐标点的梯度向量,不断让损失函数的取值J逐渐逼近最小值，再返回这个最小值对应的参数取值[𝛉1,𝛉2]的过程。</li>
</ul>
</li>
</ul>
<h3 id="正则化"><a class="markdownIt-Anchor" href="#正则化"></a> 正则化</h3>
<ul>
<li>
<p>由于我们追求损失函数的最小值，让模型在训练集上表现最优，可能会引发另一个问题:如果模型在训练集上表示优秀，却在测试集上表现糟糕，模型就会过拟合。所以我们还是需要使用控制过拟合的技术来帮助我们调整模型，对逻辑回归中过拟合的控制，通过正则化来实现。</p>
</li>
<li>
<p>正则化是用来防止模型过拟合的过程，常用的有L1正则化和L2正则化两种选项，分别通过在损失函数后加上参数向量𝛉的L1范式和L2范式的倍数来实现。这个增加的范式，被称为“正则项”，也被称为&quot;惩罚项&quot;。</p>
</li>
<li>
<p>我们知道损失函数的损失值越小（在训练集中预测值和真实值越接近）则逻辑回归模型就越有可能发生过拟合（模型只在训练集中表现的好，在测试集表现的不好）的现象。通过正则化的L1和L2范式可以加入惩罚项C来矫正模型的拟合度。因为C越小则损失函数会越大表示正则化的效力越强，参数𝛉会被逐渐压缩的越来越小。</p>
<ul>
<li>注意：L1正则化会将参数w压缩为0，L2正则化只会让参数尽量小，不会取到0。原因解释会涉及到坐标下降法和求导相关，故作不解释，只需要记住结论即可！</li>
</ul>
</li>
<li>
<p>L1和L2范式的区别</p>
<ul>
<li>在L1正则化在逐渐加强的过程中，携带信息量小的、对模型贡献不大的特征的参数w，会比携带大量信息的、对模型有巨大贡献的特征的参数更快地变成0，所以L1正则化本质是一个特征选择的过程。L1正则化越强，参数向量中就越多的参数为0，选出来的特征就越少，以此来防止过拟合。因此，如果特征量很大，数据维度很高，我们会倾向于使用L1正则化。</li>
<li>L2正则化在加强的过程中，会尽量让每个特征对模型都有一些小的贡献，但携带信息少，对模型贡献不大的特征的参数w会非常接近于0。通常来说，如果我们的主要目的只是为了防止过拟合，选择L2正则化就足够了。但是如果选择L2正则化后还是过拟合，模型在未知数据集上的效果表现很差，就可以考虑L1正则化。</li>
</ul>
</li>
</ul>
<h3 id="逻辑回归api"><a class="markdownIt-Anchor" href="#逻辑回归api"></a> 逻辑回归API</h3>
<ul>
<li>from sklearn.linear_model import LogisticRegression</li>
<li>超参数介绍：
<ul>
<li>penalty：
<ul>
<li>可以输入l1或者l2来指定使用哪一种正则化方式。不填写默认&quot;l2&quot;。 注意，若选择&quot;l1&quot;正则化，参数solver仅能够使用求解方式”liblinear&quot;和&quot;saga“，若使用“l2”正则 化，参数solver中所有的求解方式都可以使用。</li>
</ul>
</li>
<li>C：
<ul>
<li>惩罚项。C越小，损失函数会越大，模型对损失函数的惩罚越重，正则化的效力越强，参数会逐渐被压缩得越来越小。</li>
</ul>
</li>
<li>max_iter:
<ul>
<li>梯度下降中能走的最大步数，默认值为100.步数的不同取值可以帮助我们获取不同的损失函数的损失值。目前没有好的办法可以计算出最优的max_iter的值，一般是通过绘制学习曲线对其进行取值。</li>
</ul>
</li>
<li>solver：
<ul>
<li>我们之前提到的梯度下降法，只是求解逻辑回归参数𝛉的一种方法。sklearn为我们提供了多种选择，让我们可以使用不同的求解器来计算逻辑回归。求解器的选择，由参数&quot;solver&quot;控制，共有五种选择。
<ul>
<li>liblinear：是二分类专用（梯度下降），也是现在的默认求解器。</li>
<li>lbfgs,newton-cg,sag,saga:是多分类专用，几乎不用。<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%927.png" alt="img" /></li>
</ul>
</li>
<li>multi_class：</li>
</ul>
</li>
<li>输入&quot;ovr&quot;, “multinomial”, “auto&quot;来告知模型，我们要处理的分类问题的类型。默认是&quot;ovr”。
<ul>
<li>‘ovr’:表示分类问题是二分类，或让模型使用&quot;一对多&quot;的形式来处理多分类问题。</li>
<li>‘multinomial’:表示处理多分类问题，这种输入在参数solver是’liblinear’时不可用。</li>
<li>“auto”:表示会根据数据的分类情况和其他参数来确定模型要处理的分类问题的类型。比如说，如果数据是二分 类，或者solver的取值为&quot;liblinear&quot;，“auto&quot;会默认选择&quot;ovr”。反之，则会选择&quot;multinomial&quot;。</li>
</ul>
</li>
<li>class_weight:
<ul>
<li>表示样本不平衡处理的参数。样本不平衡指的是在一组数据中，某一类标签天生占有很大的比例，或误分类的代价很高，即我们想要捕捉出某种特定的分类的时候的状况。什么情况下误分类的代价很高?
<ul>
<li>例如，我们现在要对潜在犯罪者和普通人进行分类，如果没有能够识别出潜在犯罪者，那么这些人就可能去危害社会，造成犯罪，识别失败的代价会非常高，但如果，我们将普通人错误地识别成了潜在犯罪者，代价却相对较小。所以我们宁愿将普通人分类为潜在犯罪者后再人工甄别，但是却不愿将潜在犯罪者 分类为普通人，有种&quot;宁愿错杀不能放过&quot;的感觉。</li>
<li>再比如说，在银行要判断“一个新客户是否会违约”，通常不违约的人vs违约的人会是99:1的比例，真正违约的人其实是非常少的。这种分类状况下，即便模型什么也不做，全把所有人都当成不会违约的人，正确率也能有99%， 这使得模型评估指标变得毫无意义，根本无法达到我们的“要识别出会违约的人”的建模目的。</li>
</ul>
</li>
<li>None：
<ul>
<li>因此我们要使用参数class_weight对样本标签进行一定的均衡，给少量的标签更多的权重，让模型更偏向少数类， 向捕获少数类的方向建模。该参数默认None，此模式表示自动给与数据集中的所有标签相同的权重，即自动1: 1。</li>
</ul>
</li>
<li>balanced：
<ul>
<li>当误分类的代价很高的时候，我们使用”balanced“模式，可以解决样本不均衡问题。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h3>
<h4 id="乳腺癌数据集"><a class="markdownIt-Anchor" href="#乳腺癌数据集"></a> 乳腺癌数据集</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">data = datasets.load_breast_cancer()</span><br><span class="line">feature = data.data</span><br><span class="line">target = data.target</span><br><span class="line"></span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(feature,target,test_size=<span class="number">0.2</span>,random_state=<span class="number">2020</span>)</span><br><span class="line"></span><br><span class="line">model = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>,C=<span class="number">0.1</span>,max_iter=<span class="number">100</span>,</span><br><span class="line">                           solver=<span class="string">&#x27;liblinear&#x27;</span>,multi_class=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">                          class_weight=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">model.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p>0.9649122807017544</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#模型进行样本分类</span></span><br><span class="line">sample = x_test[<span class="number">6</span>]</span><br><span class="line">model.predict([sample])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算分类概率</span></span><br><span class="line">model.predict_proba([sample])</span><br></pre></td></tr></table></figure>
<p>array([[9.99999894e-01, 1.05694979e-07]])</p>
<ul>
<li>用网格搜索调整参数<br />
- Grid Search：一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果。其原理就像是在数组里找最大值。（为什么叫网格搜索？以有两个参数的模型为例，参数a有3种可能，参数b有4种可能，把所有可能性列出来，可以表示成一个3*4的表格，其中每个cell就是一个网格，循环过程就像是在每个网格里遍历、搜索，所以叫grid search）<br />
<img src="/images/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%928.png" alt="img" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV <span class="comment">#网格搜索</span></span><br><span class="line"><span class="comment">#定义一个字典：存放的就是不同的超参数的取值</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;penalty&#x27;</span>:(<span class="string">&#x27;l1&#x27;</span>,<span class="string">&#x27;l2&#x27;</span>),</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>:(<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.9</span>,<span class="number">1.0</span>),</span><br><span class="line">    <span class="string">&#x27;max_iter&#x27;</span>:(<span class="number">100</span>,<span class="number">300</span>,<span class="number">600</span>,<span class="number">800</span>,<span class="number">1000</span>,<span class="number">2000</span>,<span class="number">3000</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment">#创建网格搜索的工具对象</span></span><br><span class="line">tool = GridSearchCV(model,params,cv=<span class="number">5</span>)</span><br><span class="line"><span class="comment">#使用工具对象找寻模型最优超参数</span></span><br><span class="line">tool.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#显示最优的模型超参数</span></span><br><span class="line">tool.best_params_</span><br></pre></td></tr></table></figure>
<p>{‘C’: 1.0, ‘max_iter’: 2000, ‘penalty’: ‘l1’}</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#基于交叉验证找寻到最优的模型超参数测试模型结果为如下分数</span></span><br><span class="line">tool.best_score_</span><br></pre></td></tr></table></figure>
<p>0.956043956043956</p>
]]></content>
      <categories>
        <category>数据分析</category>
        <category>数据挖掘</category>
        <category>机器学习</category>
        <category>回归</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据挖掘</tag>
        <tag>机器学习</tag>
        <tag>回归</tag>
      </tags>
  </entry>
</search>
